{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Retrieval-Augmented Generation (RAG) & Use of Vector Store\n",
    "*by Ong Chin Ann, edited by Bernice Koh*\n",
    "\n",
    "#### Objectives\n",
    "- To understand the concept of Retrival Augmented Generation (RAG)\n",
    "- To understand the usage of text-embedding model for vectorization\n",
    "- To familarized with LangChain RAG-related components (Document Loader, Text Splitter, retrievers etc...)\n",
    "- To develop a custom chatbot with RAG approach\n",
    "\n",
    "\n",
    "If you find understanding RAG difficult, maybe you can look through the following resource after you have completed the exercise/walkthrough. <br/>\n",
    "Link: https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\n",
    "\n",
    "\n",
    "#### RAG Processes\n",
    "* Step 1: Loading and Chunking Data\n",
    "* Step 2: Construct Vector Store / Database\n",
    "* Step 3: Perform Similarity Search based on user Query\n",
    "* Step 4: Generate response from LLM based on document retrieved from the Vector Database and user query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites: Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohappyeyeballs==2.4.4 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiohttp==3.11.11 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 2)) (3.11.11)\n",
      "Requirement already satisfied: aiosignal==1.3.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.8.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 5)) (4.8.0)\n",
      "Collecting appnope==0.1.4 (from -r requirements.txt (line 6))\n",
      "  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
      "Requirement already satisfied: asttokens==3.0.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Collecting async-timeout==4.0.3 (from -r requirements.txt (line 8))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: attrs==24.3.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 9)) (24.3.0)\n",
      "Requirement already satisfied: certifi==2024.12.14 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 10)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 12)) (0.2.2)\n",
      "Requirement already satisfied: dataclasses-json==0.6.7 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 13)) (0.6.7)\n",
      "Collecting debugpy==1.8.11 (from -r requirements.txt (line 14))\n",
      "  Downloading debugpy-1.8.11-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: distro==1.9.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 16)) (1.9.0)\n",
      "Collecting docx2txt==0.8 (from -r requirements.txt (line 17))\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting exceptiongroup==1.2.2 (from -r requirements.txt (line 18))\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: executing==2.1.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 19)) (2.1.0)\n",
      "Collecting faiss-cpu==1.9.0.post1 (from -r requirements.txt (line 20))\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: frozenlist==1.5.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 21)) (1.5.0)\n",
      "Requirement already satisfied: h11==0.14.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 22)) (0.14.0)\n",
      "Requirement already satisfied: httpcore==1.0.7 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 23)) (1.0.7)\n",
      "Requirement already satisfied: httpx==0.28.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 24)) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 25)) (0.4.0)\n",
      "Requirement already satisfied: idna==3.10 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 26)) (3.10)\n",
      "Collecting importlib_metadata==8.5.0 (from -r requirements.txt (line 27))\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 28)) (6.29.5)\n",
      "Collecting ipython==8.18.1 (from -r requirements.txt (line 29))\n",
      "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: jedi==0.19.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 30)) (0.19.2)\n",
      "Requirement already satisfied: jiter==0.8.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 31)) (0.8.2)\n",
      "Requirement already satisfied: jsonpatch==1.33 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 32)) (1.33)\n",
      "Requirement already satisfied: jsonpointer==3.0.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 33)) (3.0.0)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 34)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 35)) (5.7.2)\n",
      "Requirement already satisfied: langchain==0.3.14 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 36)) (0.3.14)\n",
      "Requirement already satisfied: langchain-community==0.3.14 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 37)) (0.3.14)\n",
      "Collecting langchain-core==0.3.29 (from -r requirements.txt (line 38))\n",
      "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: langchain-openai==0.3.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 39)) (0.3.0)\n",
      "Requirement already satisfied: langchain-text-splitters==0.3.5 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 40)) (0.3.5)\n",
      "Collecting langsmith==0.2.10 (from -r requirements.txt (line 41))\n",
      "  Downloading langsmith-0.2.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 42))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: marshmallow==3.25.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 43)) (3.25.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 44)) (0.1.7)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 45))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: multidict==6.1.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 46)) (6.1.0)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 47)) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 48)) (1.6.0)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 49))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting openai==1.59.6 (from -r requirements.txt (line 50))\n",
      "  Downloading openai-1.59.6-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: orjson==3.10.14 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 51)) (3.10.14)\n",
      "Requirement already satisfied: packaging==24.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 52)) (24.2)\n",
      "Requirement already satisfied: parso==0.8.4 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 53)) (0.8.4)\n",
      "Collecting pexpect==4.9.0 (from -r requirements.txt (line 54))\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 55)) (4.3.6)\n",
      "Collecting pprintpp==0.4.0 (from -r requirements.txt (line 56))\n",
      "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 57)) (3.0.48)\n",
      "Requirement already satisfied: propcache==0.2.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 58)) (0.2.1)\n",
      "Requirement already satisfied: psutil==6.1.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 59)) (6.1.1)\n",
      "Collecting ptyprocess==0.7.0 (from -r requirements.txt (line 60))\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 61)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.10.5 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 62)) (2.10.5)\n",
      "Requirement already satisfied: pydantic-settings==2.7.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 63)) (2.7.1)\n",
      "Requirement already satisfied: pydantic_core==2.27.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 64)) (2.27.2)\n",
      "Requirement already satisfied: Pygments==2.19.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 65)) (2.19.1)\n",
      "Collecting pypdf==5.1.0 (from -r requirements.txt (line 66))\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 67)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 68)) (1.0.1)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 69)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 70)) (26.2.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 71)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 72)) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt==1.0.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 73)) (1.0.0)\n",
      "Collecting rich==13.9.4 (from -r requirements.txt (line 74))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six==1.17.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 75)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 76)) (1.3.1)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.37 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 77)) (2.0.37)\n",
      "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 78)) (0.6.3)\n",
      "Requirement already satisfied: tenacity==9.0.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 79)) (9.0.0)\n",
      "Requirement already satisfied: tiktoken==0.8.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 80)) (0.8.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 81)) (6.4.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 82)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 83)) (5.14.3)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 84)) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 85)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 86)) (2.3.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 87)) (0.2.13)\n",
      "Requirement already satisfied: yarl==1.18.3 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from -r requirements.txt (line 88)) (1.18.3)\n",
      "Collecting zipp==3.21.0 (from -r requirements.txt (line 89))\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from ipython==8.18.1->-r requirements.txt (line 29)) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from jupyter_core==5.7.2->-r requirements.txt (line 35)) (308)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\danie\\.vscode\\llm\\lib\\site-packages (from SQLAlchemy==2.0.37->-r requirements.txt (line 77)) (3.1.1)\n",
      "Downloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading debugpy-1.8.11-cp312-cp312-win_amd64.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.6/5.3 MB 16.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.3 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.3/5.3 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/13.8 MB 8.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.4/13.8 MB 9.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.2/13.8 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.6/13.8 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.2/13.8 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.5/13.8 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.8/13.8 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
      "   ---------------------------------------- 0.0/808.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 808.2/808.2 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
      "Downloading langsmith-0.2.10-py3-none-any.whl (326 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 9.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 10.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.6/15.5 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 7.1/15.5 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.3/15.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.4/15.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading openai-1.59.6-py3-none-any.whl (454 kB)\n",
      "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (pyproject.toml): started\n",
      "  Building wheel for docx2txt (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=4003 sha256=62349c49c3aa557f8d80228836b7d6239487695d3452621269afee925caa3b2d\n",
      "  Stored in directory: c:\\users\\danie\\appdata\\local\\pip\\cache\\wheels\\6f\\81\\48\\001bbc0109c15e18c009eee300022f42d1e070e54f1d00b218\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: ptyprocess, pprintpp, docx2txt, zipp, pypdf, pexpect, numpy, mdurl, exceptiongroup, debugpy, async-timeout, appnope, markdown-it-py, ipython, importlib_metadata, faiss-cpu, rich, openai, langsmith, langchain-core\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.8.12\n",
      "    Uninstalling debugpy-1.8.12:\n",
      "      Successfully uninstalled debugpy-1.8.12\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.31.0\n",
      "    Uninstalling ipython-8.31.0:\n",
      "      Successfully uninstalled ipython-8.31.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.59.7\n",
      "    Uninstalling openai-1.59.7:\n",
      "      Successfully uninstalled openai-1.59.7\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.2.11\n",
      "    Uninstalling langsmith-0.2.11:\n",
      "      Successfully uninstalled langsmith-0.2.11\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.30\n",
      "    Uninstalling langchain-core-0.3.30:\n",
      "      Successfully uninstalled langchain-core-0.3.30\n",
      "Successfully installed appnope-0.1.4 async-timeout-4.0.3 debugpy-1.8.11 docx2txt-0.8 exceptiongroup-1.2.2 faiss-cpu-1.9.0.post1 importlib_metadata-8.5.0 ipython-8.18.1 langchain-core-0.3.29 langsmith-0.2.10 markdown-it-py-3.0.0 mdurl-0.1.2 numpy-1.26.4 openai-1.59.6 pexpect-4.9.0 pprintpp-0.4.0 ptyprocess-0.7.0 pypdf-5.1.0 rich-13.9.4 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\danie\\.vscode\\LLM\\Lib\\site-packages\\~ebugpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# install all required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading required libraries\n",
    "\n",
    "Link: \n",
    "- PDF Loader: https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/\n",
    "- DOCX Loader: https://python.langchain.com/v0.1/docs/integrations/document_loaders/microsoft_word/\n",
    "- FAISS: https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/\n",
    "- AzureOpenAIEmbeddings: https://python.langchain.com/v0.1/docs/integrations/text_embedding/azureopenai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader, Docx2txtLoader, PyPDFLoader\n",
    "\n",
    "# Import .env variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: use rich for pretty printing\n",
    "from rich import print\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1a: Document Loader\n",
    "The following code shows how to load document (docx & pdf) using Docx2txtLoader & PyPDFLoader classes from LangChain\n",
    "\n",
    "Link: \n",
    "- PDF Loader: https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/\n",
    "- DOCX Loader: https://python.langchain.com/v0.1/docs/integrations/document_loaders/microsoft_word/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Docx Documents\n",
    "faq_loader = Docx2txtLoader(\"resources/docs/FAQs about the Course.docx\")\n",
    "faq = faq_loader.load()\n",
    "\n",
    "print(\"Number of Docs: \", len(faq))\n",
    "\n",
    "print(faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading PDF Documents\n",
    "info_loader = PyPDFLoader(\"resources/docs/SC1015_BasicInformation.pdf\")\n",
    "courseinfo = info_loader.load()\n",
    "\n",
    "print(\"Number of Docs: \", len(courseinfo))\n",
    "print(courseinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1b: Document Chunking & Split\n",
    "When sending a context for LLM for reasoning/inference, its best not to dump in the entire document as this will cause more tokens to be consumed and sometimes can be slow or inaccurate. Hence, there is a need to split the document into chunks which will be stored/index using Vector Store/Databases. \n",
    "\n",
    "When parsing the context, we only need to find relevant chunks from the vector store/DB and send over to LLM. This will make the response much more accurate and faster. \n",
    "\n",
    "Link: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/character_text_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Define chunk size and chunk overlap\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 400\n",
    "\n",
    "# Define text splitter\n",
    "text_spliter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split documents into chunks\n",
    "faq_chunks = text_spliter.split_documents(faq)\n",
    "courseinfo_chunks = text_spliter.split_documents(courseinfo)\n",
    "\n",
    "# Get length of chunks\n",
    "print(\"Number of FAQ Chunks: \", len(faq_chunks))\n",
    "print(\"Number of Course Info Chunks: \", len(courseinfo_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of chunks\n",
    "print(\"Chunk 1 -----------------------\")\n",
    "print(faq_chunks[0])\n",
    "print(\"\")\n",
    "print(\"Chunk 2 -----------------------\")\n",
    "print(faq_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining both documents' chunks into single collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allchunks = []\n",
    "\n",
    "allchunks.extend(courseinfo_chunks)\n",
    "allchunks.extend(faq_chunks)\n",
    "\n",
    "print(\"Number of Chunks for Course Info: \", len(courseinfo_chunks))\n",
    "print(\"Number of Chunks for FAQ: \", len(faq_chunks))\n",
    "print(\"Number of Chunks Combined Chunk for both documents: \", len(allchunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Vector Stores, Databases, & Indexing\n",
    "\n",
    "Now that the chunks from both documents are ready, we will now create an index and store them into a database called Vector Store/Vector Database (Knowledge Base). This is a special database which uses text embedding model which translate a bunch of text into vectors. \n",
    "\n",
    "This vector will be useful for search or comparison for similar text given. That's is how the relevant chunk of documents is retrieved when we pass in a query so the vector store/DB could return the relevant chunks of document which will later being passed to LLM as a context/knowledge.\n",
    "\n",
    "In this exercise, we will be using the Facebook AI Similarity Search (FAISS) as vector store while the AzureOpenAI Text Embedding Model will be use for the text to vector conversion process. Note that FAISS is considered a local database, same goes to ChromaDB while Pinecone and AzureAISearch is considered a cloud-based vector store which uses API calls.\n",
    "\n",
    "- AzureOpenAI Text Embedding Model: https://python.langchain.com/v0.1/docs/integrations/text_embedding/azureopenai/\n",
    "- Facebook AI Similarity Search (FAISS) :\n",
    "    - Vector Database\n",
    "    - Docs: https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html\n",
    "\n",
    "*__Acknowledgement__: This access key and resources are supported by NTU EdeX Teaching and Learning Grants 2023-2024 (Call 1)  from the Center for Teaching, Learning & Pedagogy (CTLP) for the project titled \"AskNarelle\". Please do not share and distribute to other parties*\n",
    "\n",
    "##### Other Vector Stores/Databases\n",
    "- ChromaDB\n",
    "    - Docs: https://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/\n",
    "- Pinecone:\n",
    "    - Docs: https://python.langchain.com/v0.1/docs/integrations/retrievers/self_query/pinecone/\n",
    "- AzureAISearch :\n",
    "    - Docs: https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Setting up text embeddings\n",
    "text_embedding =  AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    azure_deployment=os.environ[\"AZURE_TEXT_EMBEDDING\"],\n",
    "    model='text-embedding-ada-002'\n",
    ")\n",
    "\n",
    "# Creating the FAISS vector database\n",
    "faissDB = FAISS.from_documents(allchunks, text_embedding)\n",
    "print(\"Total chunks & index in FAISS vector database: \", faissDB.index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Perform Similarity Search based on user Query\n",
    "\n",
    "We have created a vector database by feeding in the combined chunks for both documents. Now, we will generate a retrieval object which will return top 3 most relavent chunks of document based on a query. \n",
    "\n",
    "Docs: https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.as_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a retriever for similarity search\n",
    "\n",
    "# Option 1: Configure to return the top k most similar chunks\n",
    "# retrieval = faissDB.as_retriever(search_kwargs={\"k\": 3}) \n",
    "\n",
    "# Option 2: Configure to retrieve documents that have a relevance score above a certain threshold\n",
    "retrieval = faissDB.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.5}\n",
    ")\n",
    "\n",
    "retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time to verify the retrieval and try to search relevant chunk of documents based on query given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = \"What is the course all about?\"\n",
    "\n",
    "# Retrieve context with retriever\n",
    "result = retrieval.invoke(query)\n",
    "print(\"Total chunks returned: \", len(result))\n",
    "print(result[0])\n",
    "\n",
    "print(\"=====================================================================\")\n",
    "print(\"--- ALL RETRIEVED CONTENT ----\")\n",
    "for r in result:\n",
    "    print(r.page_content)\n",
    "    print(\"  ### [Source Document: \", r.metadata['source'], \"]\")\n",
    "    print(\"\\n     ------------------------------------        \\n\")\n",
    "print(\"=====================================================================\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Generate response from LLM based on document retrieved and query\n",
    "\n",
    "This is the last portion where will will construct the logic of the chatbot. The procedure and flow will be as follows:\n",
    "1) Construct a system prompt\n",
    "2) Get the user's query\n",
    "3) The retriever will return relevant chunks of documents based on query given\n",
    "4) Construct the relevant chunks as the context along with user query\n",
    "5) Pass the relevant chunk and user query to LLM for inference\n",
    "6) Display the response and append into chatlog along with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import langchain\n",
    "from langchain.llms import AzureOpenAI                                          ## This object is a connector/wrapper for OpenAI LLM engine\n",
    "from langchain_openai import AzureChatOpenAI                                    ## This object is a connector/wrapper for ChatOpenAI engine\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage      ## These are the commonly used chat messages\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialise LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    azure_deployment=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"You are a teaching assistant at for the course SC1015 at NTU.\"\n",
    "task =\"your task is to answer student query about the data science and ai course.\"\n",
    "context = \"the context will be provided based on the course information and FAQ along with the user query\"\n",
    "condition = \"If user ask any query beyond data science and ai, tell the user you are not an expert of the topic the user is asking and say sorry. If you are unsure about certain query, say sorry and advise the user to contact the instructor at instructor@ntu.edu.sg\"\n",
    "### any other things to add on\n",
    "\n",
    "## Constructing initial system message\n",
    "sysmsg = f\"{persona} {task} {context} {condition}\"\n",
    "\n",
    "\n",
    "chatlog = [SystemMessage(content=sysmsg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function to perform the retrieval and consolidation of the chunks for easy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query):\n",
    "    search_result = retrieval.invoke(query)\n",
    "    context = []\n",
    "    for r in search_result:\n",
    "        context.append(r.page_content)\n",
    "\n",
    "    instruction = \"Try to understand the user query and answer based on the context given below:\\n\"\n",
    "    return SystemMessage(content=f\"{instruction}'context':{context}, 'userquery':{query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out function\n",
    "x = search_chunks(\"what is the course all about?\")\n",
    "print(x.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chatbot Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot simulation\n",
    "print(\"==== CHATBOT SIMULATION ====\")\n",
    "print(\"---- Enter 'exit' to quit\")\n",
    "\n",
    "\n",
    "query = input(\"Enter your message: \")\n",
    "usermsg = HumanMessage(content=query)\n",
    "\n",
    "# Print first system message\n",
    "print(\"System Msg:\")\n",
    "print(sysmsg)\n",
    "\n",
    "while query != \"exit\":\n",
    "    print(f\"Human : {query}\\n\")\n",
    "    \n",
    "    # Update chatlog\n",
    "    chatlog.append(usermsg)\n",
    "\n",
    "    # Retrieve relevant context\n",
    "    context = search_chunks(query)\n",
    "\n",
    "    # Print context\n",
    "    print(\"System Msg:\")\n",
    "    print(context.content)\n",
    "    \n",
    "    # Update chatlog\n",
    "    templog = chatlog + [context]\n",
    "\n",
    "    # Feed updated chatlog to LLM\n",
    "    response = llm.invoke(templog)\n",
    "\n",
    "    sleep(2)\n",
    "    print(f\"AI    : {response.content}\\n\")\n",
    "    chatlog.append(response)\n",
    "\n",
    "        \n",
    "    query = input(\"Enter your message: \")\n",
    "    usermsg = HumanMessage(content=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chatlog\n",
    "print(chatlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONGRATULATIONS!!!!!\n",
    "\n",
    "You have completed the basic RAG-LLM Based Chatbot development. In general, you have undergo the fundamental process & pipeline of creating a custom chatbot based on knowledge fed into the vector store (some people call this 'fine-tuning' ðŸ˜‚). This is one out of many ways how a custom chatbot can be constructed. It just like an art on how you can creatively design and develop the pipeline of chatbot based on available technolgies.\n",
    "\n",
    "In this exercise, we only explore try some technologies for Text Embedding Model (AzureOpenAI Text Embedding), Vector Store/Database (FAISS), and a simple way of crafting the prompts to get desire response (prompt engineering). There are more things beyond what we have covered so far awaitng your exploration. Do explore and try out things differently, and if you discover something interesting and effective, do tell me and maybe you can be the facilitator to showcase your experience on the next event/seminar/workshop/bootcamp!!!!\n",
    "\n",
    "As of now, you have learnt almost everything you need for a local chatbot development. Why dont try to create one for yourself this evening using the technologies and concept covered i.e. Prompt Engineering, LangChain, Streamlit, & RAG for your own custom chatbot development? Then tomorrow maybe we can see if you can deploy the app to Azure to make it live and publicly accessible!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Something Extra\n",
    "\n",
    "In case you wish to persist your vector store, you can actually save it locally and load at the later time so you do not need to undergo Step 1-3 anymore. Below is the simple example.\n",
    "\n",
    "Link: https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/#saving-and-loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save session's vector store locally\n",
    "faissDB.save_local(\"db/sc1015\")\n",
    "\n",
    "#  define new text embeddings\n",
    "new_text_embedding =  AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    azure_deployment=os.environ[\"AZURE_TEXT_EMBEDDING\"],\n",
    "    model='text-embedding-ada-002'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved vector store \n",
    "new_db = FAISS.load_local(\"db/sc1015\", new_text_embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "docs = new_db.similarity_search(\"What is this course all about?\", k=5)\n",
    "\n",
    "for d in docs:\n",
    "    print(d)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Extra Readings:\n",
    "- https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
