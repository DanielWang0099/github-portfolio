{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80402162",
   "metadata": {},
   "source": [
    "# What is a Large Language Model (LLM)?\n",
    "Large language models (LLMs) are artificial intelligence (AI) algorithms that analyze and understand text. They are a type of deep learning model that use neural networks to process large amounts of data. LLMs are also trained on large datasets. Do you know ChatGPT 4 training data size was said to be about 45TB of compressed plaintext. After filtering, it was ~570GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ccc52",
   "metadata": {},
   "source": [
    "![Image](img/llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f70e6",
   "metadata": {},
   "source": [
    "# Sharing Session\n",
    "\n",
    "Nicholas\n",
    "- Understand the fundamental concept of Prompt Engineering\n",
    "- Familiarize with Langchain Framework for LLM application\n",
    "\n",
    "Bernice\n",
    "- Understand and familiarize with Retrieval Augmented Generation (RAG) concepts for context-aware chatbot development\n",
    "- Construct and use Vector Databases for LLM chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf20a22",
   "metadata": {},
   "source": [
    "## Our goal for this session\n",
    "\n",
    "![Image](img/takeaway.png)\n",
    "\n",
    "1. **Introduction to LangChain and Prompt Engineering**\n",
    "\n",
    "Introduce to you what you can do with LangChain. Briefly go through the importance of prompt engineering in LLM applications.\n",
    "\n",
    "2. **Setting Up the Environment**\n",
    "\n",
    "Installation and configuration of necessary libraries. In this course, you will be provided with all ENV. In the later course, you will learn how you can provision your OpenAI API key from Microsoft Azure portal.\n",
    "\n",
    "3. **Building Prompts with LangChain**\n",
    "\n",
    "Learn how to craft effective prompts for specific tasks. We will explore different prompt techniques like zero shot, few shot, chain of thought and more. We will be implementing LangChain for real-world scenarioslike to summarise, creative writing and more.\n",
    "\n",
    "4. **Invoking LLMs Using LangChain**\n",
    "\n",
    "Learn how you can use LangChain to connect with LLMs via the Chat Models.\n",
    "\n",
    "5. **Callback for cost analysis**\n",
    "\n",
    "Creating and running prompts is fun but we will learn how we can do a callback function to get the cost for each prompt.\n",
    "   \n",
    "6. **Prompt Template & Chaining**\n",
    "\n",
    "Chaining is a sequence of calls that can be run sequential or in parallel. It combines various individual chains and the output of a chain can serve as the input for the next.\n",
    "\n",
    "7. **Agents & Tools**\n",
    "\n",
    "This can help the LLM perform search, interact with data and even execute tasks. We will be building tools and letting the LLM pick the best tool it think will help to solve the issue.\n",
    "\n",
    "8. **Calculating the Token Cost**\n",
    "\n",
    "Introduction to tiktoken which is a fast Byte Pair Encoding tokeniser developed by OpenAI which is to help us to count the token for LLM and ensure we are within the LLM token limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3727e9-a9ea-4e20-9723-0cc344fd551c",
   "metadata": {},
   "source": [
    "# Prompt Engineering & LangChain LLM Invocation\n",
    "Rapid development of AI technology has opened more applications of how we can introduce AI into processes and building of our application. This workshop will focus on building the foundational skills needed to understand and build LLM products. This includes the best practices of prompt engineering, hands-on Azure portal and introduction to Langchain framework. In addition, participants will gain practical and actionable insights to start crafting prompts that effectively harness the power of LLM. This interactive session is the fundamental session to start experimenting with Microsoft Azure OpenAI API.\n",
    "\n",
    "Takeaways:\n",
    "- Understand the fundamental concept of Prompt Engineering\n",
    "- Familiarize with Langchain Framework for LLM application\n",
    "\n",
    "# Lesson 1: Prompt Engineering\n",
    "In this lesson, you'll practice two prompting principles and their related tactics in order to write effective prompts for large language models.\n",
    "\n",
    "- **Type 1: Write clear and specific instructions**\n",
    "- **Type 2: Give the model time to “think”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5086222d-4a28-42ac-98bd-3c72ee7ca426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbc492-ae85-4a5c-81d3-258f7adbcd12",
   "metadata": {},
   "source": [
    "# Env File Example\n",
    "\n",
    "<!-- AZURE_OPENAI_ENDPOINT=\"<ENTER ENDPOINT>\"\n",
    "\n",
    "AZURE_OPENAI_API_KEY=\"<ENTER API KEY>\"\n",
    "\n",
    "AZURE_OPENAI_DEPLOYMENT=\"gpt-4o\"\n",
    "\n",
    "AZURE_OPENAI_MODEL_NAME=\"gpt-4o\"\n",
    "\n",
    "OPENAI_API_VERSION=\"2024-02-15-preview\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3820dca-3036-49c9-9609-04892aed44da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.14)\n",
      "Requirement already satisfied: langchain_openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.59.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.1.131)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.9.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain_openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d02d74e8-2372-4ffa-ad3b-c6467ea06c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print as Object: \n",
      " content=\"I don't have real-time data access to provide the current temperature in Singapore. However, Singapore typically has a tropical rainforest climate, with temperatures averaging around 25°C to 31°C (77°F to 88°F) throughout the year. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or app.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 13, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-d418f12c-e9b6-4aa7-8ba8-3a0d24424db5-0' usage_metadata={'input_tokens': 13, 'output_tokens': 69, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print as JSON: \n",
      " {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"I don't have real-time data access to provide the current temperature in Singapore. However, Singapore typically has a tropical rainforest climate, with temperatures averaging around 25°C to 31°C (77°F to 88°F) throughout the year. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or app.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 69, 'prompt_tokens': 13, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, 'type': 'ai', 'id': 'run-d418f12c-e9b6-4aa7-8ba8-3a0d24424db5-0', 'usage_metadata': {'input_tokens': 13, 'output_tokens': 69, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}\n",
      "========================================\n",
      "Print content only: \n",
      " I don't have real-time data access to provide the current temperature in Singapore. However, Singapore typically has a tropical rainforest climate, with temperatures averaging around 25°C to 31°C (77°F to 88°F) throughout the year. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or app.\n"
     ]
    }
   ],
   "source": [
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Hi, what temperature is Singapore\")\n",
    "\n",
    "## OUTPUT\n",
    "print(\"Print as Object: \\n\",response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print as JSON: \\n\",response.to_json())\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85780e3-077c-42ff-b300-bccdb84ca941",
   "metadata": {},
   "source": [
    "### Type 1: Write clear and specific instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c14f8-8999-4916-bf89-3a882ee3db30",
   "metadata": {},
   "source": [
    "\n",
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1c7dba8-884a-4f43-b27c-6923c80cc355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To achieve desired outputs from a model, provide clear and specific instructions, as longer prompts often enhance clarity and context, reducing the likelihood of irrelevant or incorrect responses.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 121, 'total_tokens': 153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-3b3b8437-f777-4fd9-aec6-0d51fa1c4e74-0' usage_metadata={'input_tokens': 121, 'output_tokens': 32, 'total_tokens': 153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " To achieve desired outputs from a model, provide clear and specific instructions, as longer prompts often enhance clarity and context, reducing the likelihood of irrelevant or incorrect responses.\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "You should express what you want a model to do by\n",
    "providing instructions that are as clear and\n",
    "specific as you can possibly make them.\n",
    "This will guide the model towards the desired output,\n",
    "and reduce the chances of receiving irrelevant \n",
    "or incorrect responses. Don't confuse writing a \n",
    "clear prompt with writing a short prompt. \n",
    "In many cases, longer prompts provide more clarity\n",
    "and context for the model, which can lead to \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt_template = f\"\"\"\n",
    "Summarize the text delimited by triple backticks\n",
    "into a single sentence.\n",
    "```{prompt_message}```\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e82c3-e145-40ea-a15d-a5e738e03bf3",
   "metadata": {},
   "source": [
    "#### Tactic 2: Ask for a structured output\n",
    "- JSON, HTML\n",
    "\n",
    "This is a great way when dealing backend such as trying to create new entries or even search for entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73bfce1f-9016-4ec4-be72-89a734507ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n[\\n    {\\n        \"book_id\": 1,\\n        \"title\": \"Whispers of the Forgotten Forest\",\\n        \"author\": \"Elena Moonshadow\",\\n        \"genre\": \"Fantasy\"\\n    },\\n    {\\n        \"book_id\": 2,\\n        \"title\": \"The Clockmaker\\'s Secret\",\\n        \"author\": \"Jasper Thorne\",\\n        \"genre\": \"Mystery\"\\n    },\\n    {\\n        \"book_id\": 3,\\n        \"title\": \"Echoes of Tomorrow\",\\n        \"author\": \"Sophie Lark\",\\n        \"genre\": \"Science Fiction\"\\n    }\\n]\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 46, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-5bfbd2e2-3e46-47ce-97f6-87a5df129ce3-0' usage_metadata={'input_tokens': 46, 'output_tokens': 128, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " ```json\n",
      "[\n",
      "    {\n",
      "        \"book_id\": 1,\n",
      "        \"title\": \"Whispers of the Forgotten Forest\",\n",
      "        \"author\": \"Elena Moonshadow\",\n",
      "        \"genre\": \"Fantasy\"\n",
      "    },\n",
      "    {\n",
      "        \"book_id\": 2,\n",
      "        \"title\": \"The Clockmaker's Secret\",\n",
      "        \"author\": \"Jasper Thorne\",\n",
      "        \"genre\": \"Mystery\"\n",
      "    },\n",
      "    {\n",
      "        \"book_id\": 3,\n",
      "        \"title\": \"Echoes of Tomorrow\",\n",
      "        \"author\": \"Sophie Lark\",\n",
      "        \"genre\": \"Science Fiction\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_template = f\"\"\"\n",
    "Generate a list of three made-up book titles along \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b3b52-3198-4e38-b095-b8d1a9c17ba1",
   "metadata": {},
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56ad27e6-109d-49c8-870b-e5a94bbfee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 1:\n",
      "content='Step 1 - Get some water boiling.  \\nStep 2 - Grab a cup and put a tea bag in it.  \\nStep 3 - Once the water is hot enough, pour it over the tea bag.  \\nStep 4 - Let it sit for a bit so the tea can steep.  \\nStep 5 - After a few minutes, take out the tea bag.  \\nStep 6 - Add sugar or milk to taste, if desired.  \\nStep 7 - Enjoy your delicious cup of tea!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 183, 'total_tokens': 287, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-a6c8bab9-fa49-4fad-8709-b18de5c12865-0' usage_metadata={'input_tokens': 183, 'output_tokens': 104, 'total_tokens': 287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Step 1 - Get some water boiling.  \n",
      "Step 2 - Grab a cup and put a tea bag in it.  \n",
      "Step 3 - Once the water is hot enough, pour it over the tea bag.  \n",
      "Step 4 - Let it sit for a bit so the tea can steep.  \n",
      "Step 5 - After a few minutes, take out the tea bag.  \n",
      "Step 6 - Add sugar or milk to taste, if desired.  \n",
      "Step 7 - Enjoy your delicious cup of tea!\n"
     ]
    }
   ],
   "source": [
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \n",
    "water boiling. While that's happening, \n",
    "grab a cup and put a tea bag in it. Once the water is\n",
    "hot enough, just pour it over the tea bag.\n",
    "Let it sit for a bit so the tea can steep. After a\n",
    "few minutes, take out the tea bag. If you \n",
    "like, you can add some sugar or milk to taste. \n",
    "And that's it! You've got yourself a delicious \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt_template = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee35f1c4-1fbe-4865-b579-4525448084aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/d9/99zh55td5557wrk09bgvss_w0000gn/T/ipykernel_8270/1311227730.py:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 2:\n",
      "content='No steps provided.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 174, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-33e6eeea-93df-4e51-8ba1-696fcf28128e-0' usage_metadata={'input_tokens': 174, 'output_tokens': 4, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " No steps provided.\n"
     ]
    }
   ],
   "source": [
    "text_2 = f\"\"\"\n",
    "The sun is shining brightly today, and the birds are \n",
    "singing. It's a beautiful day to go for a \n",
    "walk in the park. The flowers are blooming, and the \n",
    "trees are swaying gently in the breeze. People \n",
    "are out and about, enjoying the lovely weather. \n",
    "Some are having picnics, while others are playing \n",
    "games or simply relaxing on the grass. It's a \n",
    "perfect day to spend time outdoors and appreciate the \n",
    "beauty of nature.\n",
    "\"\"\"\n",
    "prompt_template = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bfc69-477f-4cd6-916f-fcd8f0752da8",
   "metadata": {},
   "source": [
    "#### Tactic 4: \"Zero-shot\" prompting\n",
    "Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14bb7bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Zero-shot prompting:\n",
      "content='In the group of numbers you provided (26, 40, 13, 50, 16, 22, 12), the only odd number is 13. \\n\\nTo check if it is divisible by 13, we can perform the division:\\n\\n13 ÷ 13 = 1\\n\\nSince 1 is a whole number, 13 is indeed divisible by 13. \\n\\nThus, the statement \"All odd numbers in this group are divisible by 13\" is true, as the only odd number in the group is 13, and it is divisible by 13.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 41, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-e0ad3b9b-4d34-4276-b7e0-5c263f8dbec4-0' usage_metadata={'input_tokens': 41, 'output_tokens': 120, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " In the group of numbers you provided (26, 40, 13, 50, 16, 22, 12), the only odd number is 13. \n",
      "\n",
      "To check if it is divisible by 13, we can perform the division:\n",
      "\n",
      "13 ÷ 13 = 1\n",
      "\n",
      "Since 1 is a whole number, 13 is indeed divisible by 13. \n",
      "\n",
      "Thus, the statement \"All odd numbers in this group are divisible by 13\" is true, as the only odd number in the group is 13, and it is divisible by 13.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = f\"\"\"\n",
    "All odd number in this group is divisible by 13: 26, 40, 13, 50, 16, 22, 12.\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(\"Completion for Zero-shot prompting:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4799c97",
   "metadata": {},
   "source": [
    "#### Tactic 5: \"Few-shot\" prompting\n",
    "Insert examples in your prompt, training the model on what you want the output to look and sound like. This technique is one of the most popular way way to invoke the LLM. However, it is still not a perfect technique, especially when dealing with more complex reasoning tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "37c61851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Few-shot prompting:\n",
      "content='The answer is False. \\n\\nIn the group of numbers provided (26, 40, 13, 50, 16, 22, 12), the only odd number is 13, which is divisible by 13, but the other numbers are not odd. Therefore, the statement that \"all odd numbers in this group are divisible by 13\" is not applicable since there are no other odd numbers to consider.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 164, 'total_tokens': 251, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-c29a9c06-0bee-4f70-97a5-a32063eea88d-0' usage_metadata={'input_tokens': 164, 'output_tokens': 87, 'total_tokens': 251, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " The answer is False. \n",
      "\n",
      "In the group of numbers provided (26, 40, 13, 50, 16, 22, 12), the only odd number is 13, which is divisible by 13, but the other numbers are not odd. Therefore, the statement that \"all odd numbers in this group are divisible by 13\" is not applicable since there are no other odd numbers to consider.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = f\"\"\"\n",
    "All even number in this group is divisible by 7: 16, 10, 14, 4, 8, 12, 24.\n",
    "A: The answer is False.\n",
    "All even number in this group is divisible by 4: 4, 8, 12, 16, 20, 24, 28.\n",
    "A: The answer is True.\n",
    "All odd number in this group is divisible by 8: 1, 15, 20, 7, 13, 5, 3.\n",
    "A: The answer is False.\n",
    "All odd number in this group is divisible by 13: 26, 40, 13, 50, 16, 22, 12.\n",
    "A: \n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(\"Completion for Few-shot prompting:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4db90d",
   "metadata": {},
   "source": [
    "chain-of-thought (CoT) prompting has been popularized to address more complex arithmetic, commonsense, and symbolic reasoning tasks. This shows Few-Shot Prompting might be able to answer how you would expect it to. However, the response might not be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fed62e45-2c07-4514-b6e9-51178bd9de70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Few-shot prompting:\n",
      "content='<grandparent>: The mighty oak stands tall after weathering countless storms; the phoenix rises anew from its ashes; the diamond, forged under immense pressure, shines brightest in the light.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 77, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-5172ecf6-0a03-40a4-8df0-b55441339aca-0' usage_metadata={'input_tokens': 77, 'output_tokens': 37, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " <grandparent>: The mighty oak stands tall after weathering countless storms; the phoenix rises anew from its ashes; the diamond, forged under immense pressure, shines brightest in the light.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \n",
    "valley flows from a modest spring; the \n",
    "grandest symphony originates from a single note; \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(\"Completion for Few-shot prompting:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7364c6-2dbe-4f94-9d10-9801c5ee4586",
   "metadata": {},
   "source": [
    "### Type 2: Give the model time to “think” \n",
    "\n",
    "#### Tactic 1: Chain-of-Thought Prompting\n",
    "chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0774e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Chain of Thought:\n",
      "Completion:\n",
      "content='Let \\\\( c \\\\) be the number of chocolates Jim initially bought, and let \\\\( s \\\\) be the number of sweets Ken initially bought.\\n\\n1. **Distribution of Chocolates:**\\n   - Jim gives half of his chocolates to Ken, so:\\n     - Jim has \\\\( \\\\frac{c}{2} \\\\) chocolates left.\\n     - Ken receives \\\\( \\\\frac{c}{2} \\\\) chocolates.\\n\\n2. **Distribution of Sweets:**\\n   - Ken gives half of his sweets to Jim, so:\\n     - Ken has \\\\( \\\\frac{s}{2} \\\\) sweets left.\\n     - Jim receives \\\\( \\\\frac{s}{2} \\\\) sweets.\\n\\n3. **Consumption:**\\n   - Jim eats 12 sweets, so he has:\\n     \\\\[\\n     \\\\frac{s}{2} - 12 \\\\text{ sweets}\\n     \\\\]\\n   - Ken eats 18 chocolates, so he has:\\n     \\\\[\\n     \\\\frac{c}{2} \\\\text{ chocolates}\\n     \\\\]\\n\\n4. **Ratios:**\\n   - The ratio of Jim’s sweets to chocolates becomes \\\\( 1:7 \\\\):\\n     \\\\[\\n     \\\\frac{\\\\frac{s}{2} - 12}{\\\\frac{c}{2}} = \\\\frac{1}{7}\\n     \\\\]\\n     Cross-multiplying gives:\\n     \\\\[\\n     7\\\\left(\\\\frac{s}{2} - 12\\\\right) = \\\\frac{c}{2}\\n     \\\\]\\n     Simplifying:\\n     \\\\[\\n     7\\\\frac{s}{2} - 84 = \\\\frac{c}{2}\\n     \\\\]\\n     Multiplying through by 2:\\n     \\\\[\\n     7s - 168 = c \\\\quad \\\\text{(1)}\\n     \\\\]\\n\\n   - The ratio of Ken’s sweets to chocolates becomes \\\\( 1:4 \\\\):\\n     \\\\[\\n     \\\\frac{\\\\frac{s}{2}}{\\\\frac{c}{2}} = \\\\frac{1}{4}\\n     \\\\]\\n     Cross-multiplying gives:\\n     \\\\[\\n     4\\\\left(\\\\frac{s}{2}\\\\right) = \\\\frac{c}{2}\\n     \\\\]\\n     Simplifying:\\n     \\\\[\\n     2s = \\\\frac{c}{2}\\n     \\\\]\\n     Multiplying through by 2:\\n     \\\\[\\n     4s = c \\\\quad \\\\text{(2)}\\n     \\\\]\\n\\n5. **Substituting Equations:**\\n   - Substitute equation (2) into equation (1):\\n     \\\\[\\n     7s - 168 = 4s\\n     \\\\]\\n     Rearranging gives:\\n     \\\\[\\n     7s - 4s = 168\\n     \\\\]\\n     \\\\[\\n     3s = 168\\n     \\\\]\\n     \\\\[\\n     s = 56\\n     \\\\]\\n\\n6. **Finding \\\\( c \\\\):**\\n   - Substitute \\\\( s = 56 \\\\) back into equation (2):\\n     \\\\[\\n     c = 4s = 4 \\\\times 56 = 224\\n     \\\\]\\n\\nThus, Ken bought \\\\( \\\\boxed{56} \\\\) sweets.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 649, 'prompt_tokens': 83, 'total_tokens': 732, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-a94e8a2b-e080-4cd0-bb77-8333d6ecb58e-0' usage_metadata={'input_tokens': 83, 'output_tokens': 649, 'total_tokens': 732, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Let \\( c \\) be the number of chocolates Jim initially bought, and let \\( s \\) be the number of sweets Ken initially bought.\n",
      "\n",
      "1. **Distribution of Chocolates:**\n",
      "   - Jim gives half of his chocolates to Ken, so:\n",
      "     - Jim has \\( \\frac{c}{2} \\) chocolates left.\n",
      "     - Ken receives \\( \\frac{c}{2} \\) chocolates.\n",
      "\n",
      "2. **Distribution of Sweets:**\n",
      "   - Ken gives half of his sweets to Jim, so:\n",
      "     - Ken has \\( \\frac{s}{2} \\) sweets left.\n",
      "     - Jim receives \\( \\frac{s}{2} \\) sweets.\n",
      "\n",
      "3. **Consumption:**\n",
      "   - Jim eats 12 sweets, so he has:\n",
      "     \\[\n",
      "     \\frac{s}{2} - 12 \\text{ sweets}\n",
      "     \\]\n",
      "   - Ken eats 18 chocolates, so he has:\n",
      "     \\[\n",
      "     \\frac{c}{2} \\text{ chocolates}\n",
      "     \\]\n",
      "\n",
      "4. **Ratios:**\n",
      "   - The ratio of Jim’s sweets to chocolates becomes \\( 1:7 \\):\n",
      "     \\[\n",
      "     \\frac{\\frac{s}{2} - 12}{\\frac{c}{2}} = \\frac{1}{7}\n",
      "     \\]\n",
      "     Cross-multiplying gives:\n",
      "     \\[\n",
      "     7\\left(\\frac{s}{2} - 12\\right) = \\frac{c}{2}\n",
      "     \\]\n",
      "     Simplifying:\n",
      "     \\[\n",
      "     7\\frac{s}{2} - 84 = \\frac{c}{2}\n",
      "     \\]\n",
      "     Multiplying through by 2:\n",
      "     \\[\n",
      "     7s - 168 = c \\quad \\text{(1)}\n",
      "     \\]\n",
      "\n",
      "   - The ratio of Ken’s sweets to chocolates becomes \\( 1:4 \\):\n",
      "     \\[\n",
      "     \\frac{\\frac{s}{2}}{\\frac{c}{2}} = \\frac{1}{4}\n",
      "     \\]\n",
      "     Cross-multiplying gives:\n",
      "     \\[\n",
      "     4\\left(\\frac{s}{2}\\right) = \\frac{c}{2}\n",
      "     \\]\n",
      "     Simplifying:\n",
      "     \\[\n",
      "     2s = \\frac{c}{2}\n",
      "     \\]\n",
      "     Multiplying through by 2:\n",
      "     \\[\n",
      "     4s = c \\quad \\text{(2)}\n",
      "     \\]\n",
      "\n",
      "5. **Substituting Equations:**\n",
      "   - Substitute equation (2) into equation (1):\n",
      "     \\[\n",
      "     7s - 168 = 4s\n",
      "     \\]\n",
      "     Rearranging gives:\n",
      "     \\[\n",
      "     7s - 4s = 168\n",
      "     \\]\n",
      "     \\[\n",
      "     3s = 168\n",
      "     \\]\n",
      "     \\[\n",
      "     s = 56\n",
      "     \\]\n",
      "\n",
      "6. **Finding \\( c \\):**\n",
      "   - Substitute \\( s = 56 \\) back into equation (2):\n",
      "     \\[\n",
      "     c = 4s = 4 \\times 56 = 224\n",
      "     \\]\n",
      "\n",
      "Thus, Ken bought \\( \\boxed{56} \\) sweets.\n"
     ]
    }
   ],
   "source": [
    "# Prompt without Chain of Thought\n",
    "prompt_no_cot = f\"\"\"\n",
    "Jim bought some chocolates and gave half of them to Ken. \n",
    "Ken bought some sweets and gave half of them to Jim. Jim ate 12 sweets and Ken ate 18 chocolates. \n",
    "The ratio of Jim’s sweets to chocolates becomes 1:7 and the ratio of Ken’s sweets to chocolates \n",
    "becomes 1:4. How many sweets did Ken buy?\n",
    "\"\"\"\n",
    "response_no_cot = llm.invoke(prompt_no_cot)\n",
    "\n",
    "print(\"Without Chain of Thought:\")\n",
    "print(\"Completion:\")\n",
    "print(response_no_cot)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\", response_no_cot.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c40b83ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With Chain of Thought:\n",
      "Completion:\n",
      "content=\"Let's break down the problem step by step.\\n\\n1. **Let’s define variables:**\\n   - Let \\\\( C \\\\) be the number of chocolates Jim initially bought.\\n   - Let \\\\( S \\\\) be the number of sweets Ken bought.\\n\\n2. **Jim gives half of his chocolates to Ken:**\\n   - Jim has \\\\( \\\\frac{C}{2} \\\\) chocolates left.\\n   - Ken receives \\\\( \\\\frac{C}{2} \\\\) chocolates.\\n\\n3. **Ken gives half of his sweets to Jim:**\\n   - Ken has \\\\( S \\\\) sweets and gives \\\\( \\\\frac{S}{2} \\\\) to Jim.\\n   - Ken has \\\\( \\\\frac{S}{2} \\\\) sweets left.\\n   - Jim now has \\\\( \\\\frac{S}{2} \\\\) sweets in addition to his chocolates.\\n\\n4. **After eating:**\\n   - Jim eats 12 sweets, so he has \\\\( \\\\frac{S}{2} - 12 \\\\) sweets left.\\n   - Ken eats 18 chocolates, so he has \\\\( \\\\frac{C}{2} \\\\) chocolates left.\\n\\n5. **Ratios:**\\n   - The ratio of Jim’s sweets to chocolates becomes 1:7:\\n     \\\\[\\n     \\\\frac{\\\\frac{S}{2} - 12}{\\\\frac{C}{2}} = \\\\frac{1}{7}\\n     \\\\]\\n     Cross-multiplying gives:\\n     \\\\[\\n     7\\\\left(\\\\frac{S}{2} - 12\\\\right) = \\\\frac{C}{2}\\n     \\\\]\\n     Simplifying:\\n     \\\\[\\n     7\\\\frac{S}{2} - 84 = \\\\frac{C}{2}\\n     \\\\]\\n     Multiplying through by 2:\\n     \\\\[\\n     7S - 168 = C \\\\quad \\\\text{(Equation 1)}\\n     \\\\]\\n\\n   - The ratio of Ken’s sweets to chocolates becomes 1:4:\\n     \\\\[\\n     \\\\frac{\\\\frac{S}{2}}{\\\\frac{C}{2}} = \\\\frac{1}{4}\\n     \\\\]\\n     Cross-multiplying gives:\\n     \\\\[\\n     4\\\\left(\\\\frac{S}{2}\\\\right) = \\\\frac{C}{2}\\n     \\\\]\\n     Simplifying:\\n     \\\\[\\n     2S = \\\\frac{C}{2}\\n     \\\\]\\n     Multiplying through by 2:\\n     \\\\[\\n     4S = C \\\\quad \\\\text{(Equation 2)}\\n     \\\\]\\n\\n6. **Substituting Equation 2 into Equation 1:**\\n   \\\\[\\n   7S - 168 = 4S\\n   \\\\]\\n   Rearranging gives:\\n   \\\\[\\n   7S - 4S = 168\\n   \\\\]\\n   \\\\[\\n   3S = 168\\n   \\\\]\\n   \\\\[\\n   S = 56\\n   \\\\]\\n\\nSo, Ken bought **56 sweets**.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 616, 'prompt_tokens': 189, 'total_tokens': 805, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-1259ee39-627b-4c74-9e8d-972d92cf163c-0' usage_metadata={'input_tokens': 189, 'output_tokens': 616, 'total_tokens': 805, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Let's break down the problem step by step.\n",
      "\n",
      "1. **Let’s define variables:**\n",
      "   - Let \\( C \\) be the number of chocolates Jim initially bought.\n",
      "   - Let \\( S \\) be the number of sweets Ken bought.\n",
      "\n",
      "2. **Jim gives half of his chocolates to Ken:**\n",
      "   - Jim has \\( \\frac{C}{2} \\) chocolates left.\n",
      "   - Ken receives \\( \\frac{C}{2} \\) chocolates.\n",
      "\n",
      "3. **Ken gives half of his sweets to Jim:**\n",
      "   - Ken has \\( S \\) sweets and gives \\( \\frac{S}{2} \\) to Jim.\n",
      "   - Ken has \\( \\frac{S}{2} \\) sweets left.\n",
      "   - Jim now has \\( \\frac{S}{2} \\) sweets in addition to his chocolates.\n",
      "\n",
      "4. **After eating:**\n",
      "   - Jim eats 12 sweets, so he has \\( \\frac{S}{2} - 12 \\) sweets left.\n",
      "   - Ken eats 18 chocolates, so he has \\( \\frac{C}{2} \\) chocolates left.\n",
      "\n",
      "5. **Ratios:**\n",
      "   - The ratio of Jim’s sweets to chocolates becomes 1:7:\n",
      "     \\[\n",
      "     \\frac{\\frac{S}{2} - 12}{\\frac{C}{2}} = \\frac{1}{7}\n",
      "     \\]\n",
      "     Cross-multiplying gives:\n",
      "     \\[\n",
      "     7\\left(\\frac{S}{2} - 12\\right) = \\frac{C}{2}\n",
      "     \\]\n",
      "     Simplifying:\n",
      "     \\[\n",
      "     7\\frac{S}{2} - 84 = \\frac{C}{2}\n",
      "     \\]\n",
      "     Multiplying through by 2:\n",
      "     \\[\n",
      "     7S - 168 = C \\quad \\text{(Equation 1)}\n",
      "     \\]\n",
      "\n",
      "   - The ratio of Ken’s sweets to chocolates becomes 1:4:\n",
      "     \\[\n",
      "     \\frac{\\frac{S}{2}}{\\frac{C}{2}} = \\frac{1}{4}\n",
      "     \\]\n",
      "     Cross-multiplying gives:\n",
      "     \\[\n",
      "     4\\left(\\frac{S}{2}\\right) = \\frac{C}{2}\n",
      "     \\]\n",
      "     Simplifying:\n",
      "     \\[\n",
      "     2S = \\frac{C}{2}\n",
      "     \\]\n",
      "     Multiplying through by 2:\n",
      "     \\[\n",
      "     4S = C \\quad \\text{(Equation 2)}\n",
      "     \\]\n",
      "\n",
      "6. **Substituting Equation 2 into Equation 1:**\n",
      "   \\[\n",
      "   7S - 168 = 4S\n",
      "   \\]\n",
      "   Rearranging gives:\n",
      "   \\[\n",
      "   7S - 4S = 168\n",
      "   \\]\n",
      "   \\[\n",
      "   3S = 168\n",
      "   \\]\n",
      "   \\[\n",
      "   S = 56\n",
      "   \\]\n",
      "\n",
      "So, Ken bought **56 sweets**.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt with Chain of Thought\n",
    "prompt_with_cot = f\"\"\"\n",
    "Q: Let's think step by step. Tim has 10 tennis ball. He buys 2 cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does Tim have now?\n",
    "A: Tim started with 10 tennis balls. He bought 2 cans of 3 tennis ball each.\n",
    "He bought a total of 2 * 3 = 6 tennis balls. \n",
    "Tim has 10 + 6 = 16 tennis balls now.\n",
    "\n",
    "Q: Let's think step by step.\n",
    "Jim bought some chocolates and gave half of them to Ken. \n",
    "Ken bought some sweets and gave half of them to Jim. \n",
    "Jim ate 12 sweets and Ken ate 18 chocolates. \n",
    "The ratio of Jim’s sweets to chocolates becomes 1:7 and the ratio of Ken’s sweets to chocolates \n",
    "becomes 1:4. \n",
    "How many sweets did Ken buy?\n",
    "A:\n",
    "\"\"\"\n",
    "response_with_cot = llm.invoke(prompt_with_cot)\n",
    "\n",
    "print(\"\\nWith Chain of Thought:\")\n",
    "print(\"Completion:\")\n",
    "print(response_with_cot)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\", response_with_cot.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21d52f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With Chain of Thought:\n",
      "Completion:\n",
      "content=\"Let's break down the problem step by step using the information provided.\\n\\n1. **Initial Variables**:\\n   - Let \\\\( c \\\\) be the number of chocolates Jim initially bought.\\n   - Let \\\\( s \\\\) be the number of sweets Ken initially bought.\\n\\n2. **After Sharing**:\\n   - Jim gives half of his chocolates to Ken, so Jim has \\\\( \\\\frac{c}{2} \\\\) chocolates left.\\n   - Ken receives \\\\( \\\\frac{c}{2} \\\\) chocolates from Jim, so Ken has \\\\( \\\\frac{c}{2} + \\\\frac{s}{2} \\\\) sweets after giving half of his sweets to Jim.\\n\\n3. **After Eating**:\\n   - Jim eats 12 sweets, so he has \\\\( \\\\frac{s}{2} - 12 \\\\) sweets left.\\n   - Ken eats 18 chocolates, so he has \\\\( \\\\frac{c}{2} - 18 \\\\) chocolates left.\\n\\n4. **Ratios**:\\n   - The ratio of Jim’s sweets to chocolates becomes \\\\( 1:7 \\\\):\\n     \\\\[\\n     \\\\frac{\\\\frac{s}{2} - 12}{\\\\frac{c}{2}} = \\\\frac{1}{7}\\n     \\\\]\\n     Cross-multiplying gives:\\n     \\\\[\\n     7\\\\left(\\\\frac{s}{2} - 12\\\\right) = \\\\frac{c}{2}\\n     \\\\]\\n     Simplifying this:\\n     \\\\[\\n     7\\\\frac{s}{2} - 84 = \\\\frac{c}{2}\\n     \\\\]\\n     Multiplying through by 2 to eliminate the fractions:\\n     \\\\[\\n     7s - 168 = c \\\\quad \\\\text{(Equation 1)}\\n     \\\\]\\n\\n   - The ratio of Ken’s sweets to chocolates becomes \\\\( 1:4 \\\\):\\n     \\\\[\\n     \\\\frac{\\\\frac{s}{2}}{\\\\frac{c}{2} - 18} = \\\\frac{1}{4}\\n     \\\\]\\n     Cross-multiplying gives:\\n     \\\\[\\n     4\\\\left(\\\\frac{s}{2}\\\\right) = \\\\frac{c}{2} - 18\\n     \\\\]\\n     Simplifying this:\\n     \\\\[\\n     2s = \\\\frac{c}{2} - 18\\n     \\\\]\\n     Multiplying through by 2 to eliminate the fractions:\\n     \\\\[\\n     4s = c - 36 \\\\quad \\\\text{(Equation 2)}\\n     \\\\]\\n\\n5. **Solving the Equations**:\\n   Now we have two equations:\\n   - \\\\( c = 7s - 168 \\\\) (Equation 1)\\n   - \\\\( c = 4s + 36 \\\\) (Equation 2)\\n\\n   Setting the two expressions for \\\\( c \\\\) equal to each other:\\n   \\\\[\\n   7s - 168 = 4s + 36\\n   \\\\]\\n   Rearranging gives:\\n   \\\\[\\n   7s - 4s = 36 + 168\\n   \\\\]\\n   \\\\[\\n   3s = 204\\n   \\\\]\\n   \\\\[\\n   s = 68\\n   \\\\]\\n\\n6. **Conclusion**:\\n   Ken initially bought **68 sweets**.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 670, 'prompt_tokens': 162, 'total_tokens': 832, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-3e8c18e2-d52b-4af4-93c8-be783120b85c-0' usage_metadata={'input_tokens': 162, 'output_tokens': 670, 'total_tokens': 832, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Let's break down the problem step by step using the information provided.\n",
      "\n",
      "1. **Initial Variables**:\n",
      "   - Let \\( c \\) be the number of chocolates Jim initially bought.\n",
      "   - Let \\( s \\) be the number of sweets Ken initially bought.\n",
      "\n",
      "2. **After Sharing**:\n",
      "   - Jim gives half of his chocolates to Ken, so Jim has \\( \\frac{c}{2} \\) chocolates left.\n",
      "   - Ken receives \\( \\frac{c}{2} \\) chocolates from Jim, so Ken has \\( \\frac{c}{2} + \\frac{s}{2} \\) sweets after giving half of his sweets to Jim.\n",
      "\n",
      "3. **After Eating**:\n",
      "   - Jim eats 12 sweets, so he has \\( \\frac{s}{2} - 12 \\) sweets left.\n",
      "   - Ken eats 18 chocolates, so he has \\( \\frac{c}{2} - 18 \\) chocolates left.\n",
      "\n",
      "4. **Ratios**:\n",
      "   - The ratio of Jim’s sweets to chocolates becomes \\( 1:7 \\):\n",
      "     \\[\n",
      "     \\frac{\\frac{s}{2} - 12}{\\frac{c}{2}} = \\frac{1}{7}\n",
      "     \\]\n",
      "     Cross-multiplying gives:\n",
      "     \\[\n",
      "     7\\left(\\frac{s}{2} - 12\\right) = \\frac{c}{2}\n",
      "     \\]\n",
      "     Simplifying this:\n",
      "     \\[\n",
      "     7\\frac{s}{2} - 84 = \\frac{c}{2}\n",
      "     \\]\n",
      "     Multiplying through by 2 to eliminate the fractions:\n",
      "     \\[\n",
      "     7s - 168 = c \\quad \\text{(Equation 1)}\n",
      "     \\]\n",
      "\n",
      "   - The ratio of Ken’s sweets to chocolates becomes \\( 1:4 \\):\n",
      "     \\[\n",
      "     \\frac{\\frac{s}{2}}{\\frac{c}{2} - 18} = \\frac{1}{4}\n",
      "     \\]\n",
      "     Cross-multiplying gives:\n",
      "     \\[\n",
      "     4\\left(\\frac{s}{2}\\right) = \\frac{c}{2} - 18\n",
      "     \\]\n",
      "     Simplifying this:\n",
      "     \\[\n",
      "     2s = \\frac{c}{2} - 18\n",
      "     \\]\n",
      "     Multiplying through by 2 to eliminate the fractions:\n",
      "     \\[\n",
      "     4s = c - 36 \\quad \\text{(Equation 2)}\n",
      "     \\]\n",
      "\n",
      "5. **Solving the Equations**:\n",
      "   Now we have two equations:\n",
      "   - \\( c = 7s - 168 \\) (Equation 1)\n",
      "   - \\( c = 4s + 36 \\) (Equation 2)\n",
      "\n",
      "   Setting the two expressions for \\( c \\) equal to each other:\n",
      "   \\[\n",
      "   7s - 168 = 4s + 36\n",
      "   \\]\n",
      "   Rearranging gives:\n",
      "   \\[\n",
      "   7s - 4s = 36 + 168\n",
      "   \\]\n",
      "   \\[\n",
      "   3s = 204\n",
      "   \\]\n",
      "   \\[\n",
      "   s = 68\n",
      "   \\]\n",
      "\n",
      "6. **Conclusion**:\n",
      "   Ken initially bought **68 sweets**.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt with Chain of Thought\n",
    "prompt_with_cot = f\"\"\"\n",
    "Let's think step by step.\n",
    "Jim bought some chocolates and gave half of them to Ken. \n",
    "Ken bought some sweets and gave half of them to Jim. Jim ate 12 sweets and Ken ate 18 chocolates. \n",
    "The ratio of Jim’s sweets to chocolates becomes 1:7 and the ratio of Ken’s sweets to chocolates \n",
    "becomes 1:4. \n",
    "\n",
    "To solve this:\n",
    "1. Let the number of chocolates Jim initially bought be c.\n",
    "2. Let the number of sweets Ken initially bought be s.\n",
    "3. After giving away, Jim has c/2 chocolates and (s/2 - 12) sweets.\n",
    "4. Ken has (c/2 - 18) chocolates and s/2 sweets.\n",
    "\n",
    "How many sweets did Ken buy?\n",
    "\"\"\"\n",
    "response_with_cot = llm.invoke(prompt_with_cot)\n",
    "\n",
    "print(\"\\nWith Chain of Thought:\")\n",
    "print(\"Completion:\")\n",
    "print(response_with_cot)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\", response_with_cot.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ea695",
   "metadata": {},
   "source": [
    "#### Tactic 2: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f8aa9f4-e9f2-489c-b9ae-74d3d255a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content='Jack and Jill, siblings from a charming village, embarked on a quest for water but faced misfortune when Jack fell down a hill, yet they returned home undeterred and continued their adventures. \\n\\nJack et Jill, des frères et sœurs d\\'un charmant village, se sont lancés dans une quête d\\'eau mais ont rencontré un malheur lorsque Jack est tombé d\\'une colline, mais ils sont rentrés chez eux sans se décourager et ont continué leurs aventures.\\n\\nJack, Jill\\n\\n{\\n  \"french_summary\": \"Jack et Jill, des frères et sœurs d\\'un charmant village, se sont lancés dans une quête d\\'eau mais ont rencontré un malheur lorsque Jack est tombé d\\'une colline, mais ils sont rentrés chez eux sans se décourager et ont continué leurs aventures.\",\\n  \"num_names\": 2\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 177, 'total_tokens': 356, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-1e118f63-64ba-40a3-b50e-c06d23eed82e-0' usage_metadata={'input_tokens': 177, 'output_tokens': 179, 'total_tokens': 356, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Jack and Jill, siblings from a charming village, embarked on a quest for water but faced misfortune when Jack fell down a hill, yet they returned home undeterred and continued their adventures. \n",
      "\n",
      "Jack et Jill, des frères et sœurs d'un charmant village, se sont lancés dans une quête d'eau mais ont rencontré un malheur lorsque Jack est tombé d'une colline, mais ils sont rentrés chez eux sans se décourager et ont continué leurs aventures.\n",
      "\n",
      "Jack, Jill\n",
      "\n",
      "{\n",
      "  \"french_summary\": \"Jack et Jill, des frères et sœurs d'un charmant village, se sont lancés dans une quête d'eau mais ont rencontré un malheur lorsque Jack est tombé d'une colline, mais ils sont rentrés chez eux sans se décourager et ont continué leurs aventures.\",\n",
      "  \"num_names\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on\n",
    "a quest to fetch water from a hilltop\n",
    "well. As they climbed, singing joyfully, misfortune\n",
    "struck—Jack tripped on a stone and tumbled\n",
    "down the hill, with Jill following suit.\n",
    "Though slightly battered, the pair returned home to\n",
    "comforting embraces. Despite the mishap,\n",
    "their adventurous spirits remained undimmed, and they \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_template_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template_1)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2568d43-a0cd-4ba3-a4b9-340961f5f249",
   "metadata": {},
   "source": [
    "#### Ask for output in a specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31724e2e-9c8b-4378-8e9b-d3afdd366b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content='Summary: Siblings Jack and Jill embark on a joyful quest to fetch water but face misfortune when Jack falls down the hill, yet they return home undeterred and continue their adventures.  \\nTranslation: Les frères et sœurs Jack et Jill se lancent dans une quête joyeuse pour chercher de l\\'eau, mais rencontrent un malheur lorsque Jack tombe de la colline, mais ils rentrent chez eux sans se laisser décourager et continuent leurs aventures.  \\nNames: Jack, Jill  \\nOutput JSON: {\"french_summary\":\"Les frères et sœurs Jack et Jill se lancent dans une quête joyeuse pour chercher de l\\'eau, mais rencontrent un malheur lorsque Jack tombe de la colline, mais ils rentrent chez eux sans se laisser décourager et continuent leurs aventures.\",\"num_names\":2}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 215, 'total_tokens': 386, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-7c90b85a-60c5-438e-8439-6507cc1c44ea-0' usage_metadata={'input_tokens': 215, 'output_tokens': 171, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Summary: Siblings Jack and Jill embark on a joyful quest to fetch water but face misfortune when Jack falls down the hill, yet they return home undeterred and continue their adventures.  \n",
      "Translation: Les frères et sœurs Jack et Jill se lancent dans une quête joyeuse pour chercher de l'eau, mais rencontrent un malheur lorsque Jack tombe de la colline, mais ils rentrent chez eux sans se laisser décourager et continuent leurs aventures.  \n",
      "Names: Jack, Jill  \n",
      "Output JSON: {\"french_summary\":\"Les frères et sœurs Jack et Jill se lancent dans une quête joyeuse pour chercher de l'eau, mais rencontrent un malheur lorsque Jack tombe de la colline, mais ils rentrent chez eux sans se laisser décourager et continuent leurs aventures.\",\"num_names\":2}\n"
     ]
    }
   ],
   "source": [
    "prompt_template_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template_2)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d93eb7-f5f2-47a2-a34a-48d229316fe9",
   "metadata": {},
   "source": [
    "#### Tactic 3: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf661ec4-23dd-4895-a9e5-7991eaab74ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content=\"The student's solution is mostly correct, but there is a small error in the calculation of the maintenance cost. Let's break it down step by step:\\n\\n1. **Land cost**: The cost of land is $100 per square foot, so for \\\\( x \\\\) square feet, the cost is:\\n   \\\\[\\n   100x\\n   \\\\]\\n\\n2. **Solar panel cost**: The cost of solar panels is $250 per square foot, so for \\\\( x \\\\) square feet, the cost is:\\n   \\\\[\\n   250x\\n   \\\\]\\n\\n3. **Maintenance cost**: The maintenance cost consists of a flat fee of $100,000 plus an additional $10 per square foot. Therefore, for \\\\( x \\\\) square feet, the maintenance cost is:\\n   \\\\[\\n   100,000 + 10x\\n   \\\\]\\n\\nNow, let's combine all these costs to find the total cost for the first year of operations:\\n\\n\\\\[\\n\\\\text{Total cost} = \\\\text{Land cost} + \\\\text{Solar panel cost} + \\\\text{Maintenance cost}\\n\\\\]\\n\\\\[\\n\\\\text{Total cost} = 100x + 250x + (100,000 + 10x)\\n\\\\]\\n\\\\[\\n\\\\text{Total cost} = 100x + 250x + 100,000 + 10x\\n\\\\]\\n\\\\[\\n\\\\text{Total cost} = (100x + 250x + 10x) + 100,000\\n\\\\]\\n\\\\[\\n\\\\text{Total cost} = 360x + 100,000\\n\\\\]\\n\\nThe student's final expression was \\\\( 450x + 100,000 \\\\), which is incorrect. The correct total cost for the first year of operations as a function of the number of square feet \\\\( x \\\\) is:\\n\\n\\\\[\\n\\\\text{Total cost} = 360x + 100,000\\n\\\\]\\n\\nSo, the student's solution is incorrect.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 404, 'prompt_tokens': 192, 'total_tokens': 596, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-69fc994a-c4ca-45bd-b077-5d3ec3ea65c8-0' usage_metadata={'input_tokens': 192, 'output_tokens': 404, 'total_tokens': 596, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " The student's solution is mostly correct, but there is a small error in the calculation of the maintenance cost. Let's break it down step by step:\n",
      "\n",
      "1. **Land cost**: The cost of land is $100 per square foot, so for \\( x \\) square feet, the cost is:\n",
      "   \\[\n",
      "   100x\n",
      "   \\]\n",
      "\n",
      "2. **Solar panel cost**: The cost of solar panels is $250 per square foot, so for \\( x \\) square feet, the cost is:\n",
      "   \\[\n",
      "   250x\n",
      "   \\]\n",
      "\n",
      "3. **Maintenance cost**: The maintenance cost consists of a flat fee of $100,000 plus an additional $10 per square foot. Therefore, for \\( x \\) square feet, the maintenance cost is:\n",
      "   \\[\n",
      "   100,000 + 10x\n",
      "   \\]\n",
      "\n",
      "Now, let's combine all these costs to find the total cost for the first year of operations:\n",
      "\n",
      "\\[\n",
      "\\text{Total cost} = \\text{Land cost} + \\text{Solar panel cost} + \\text{Maintenance cost}\n",
      "\\]\n",
      "\\[\n",
      "\\text{Total cost} = 100x + 250x + (100,000 + 10x)\n",
      "\\]\n",
      "\\[\n",
      "\\text{Total cost} = 100x + 250x + 100,000 + 10x\n",
      "\\]\n",
      "\\[\n",
      "\\text{Total cost} = (100x + 250x + 10x) + 100,000\n",
      "\\]\n",
      "\\[\n",
      "\\text{Total cost} = 360x + 100,000\n",
      "\\]\n",
      "\n",
      "The student's final expression was \\( 450x + 100,000 \\), which is incorrect. The correct total cost for the first year of operations as a function of the number of square feet \\( x \\) is:\n",
      "\n",
      "\\[\n",
      "\\text{Total cost} = 360x + 100,000\n",
      "\\]\n",
      "\n",
      "So, the student's solution is incorrect.\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \n",
    "me a flat $100k per year, and an additional $10 / square \n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b0bf2-2e48-4d76-a170-8b3b1c3943d2",
   "metadata": {},
   "source": [
    "#### Note that the student's solution is actually not correct.\n",
    "#### We can fix this by instructing the model to work out its own solution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "22b24fca-b49b-43d8-8e81-a9260d912fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content=\"To solve the problem, let's break down the costs step by step.\\n\\n1. **Land cost**: The cost of land is $100 per square foot. If we let \\\\( x \\\\) be the size of the installation in square feet, then the land cost is:\\n   \\\\[\\n   \\\\text{Land cost} = 100x\\n   \\\\]\\n\\n2. **Solar panel cost**: The cost of solar panels is $250 per square foot. Therefore, the solar panel cost is:\\n   \\\\[\\n   \\\\text{Solar panel cost} = 250x\\n   \\\\]\\n\\n3. **Maintenance cost**: The maintenance cost consists of a flat fee of $100,000 plus an additional $10 per square foot. Thus, the maintenance cost is:\\n   \\\\[\\n   \\\\text{Maintenance cost} = 100,000 + 10x\\n   \\\\]\\n\\nNow, we can combine all these costs to find the total cost for the first year of operations:\\n\\\\[\\n\\\\text{Total cost} = \\\\text{Land cost} + \\\\text{Solar panel cost} + \\\\text{Maintenance cost}\\n\\\\]\\nSubstituting the expressions we derived:\\n\\\\[\\n\\\\text{Total cost} = 100x + 250x + (100,000 + 10x)\\n\\\\]\\nCombining like terms:\\n\\\\[\\n\\\\text{Total cost} = (100x + 250x + 10x) + 100,000 = 360x + 100,000\\n\\\\]\\n\\nSo, the total cost for the first year of operations as a function of the number of square feet \\\\( x \\\\) is:\\n\\\\[\\n\\\\text{Total cost} = 360x + 100,000\\n\\\\]\\n\\nNow, let's compare this with the student's solution.\\n\\nStudent's solution:\\n```\\nLet x be the size of the installation in square feet.\\nCosts:\\n1. Land cost: 100x\\n2. Solar panel cost: 250x\\n3. Maintenance cost: 100,000 + 100x\\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\\n```\\n\\nThe student calculated the maintenance cost incorrectly. They used $100 per square foot instead of the correct $10 per square foot. Therefore, the total cost they calculated is incorrect.\\n\\nIs the student's solution the same as actual solution just calculated:\\n```\\nno\\n```\\nStudent grade:\\n```\\nincorrect\\n```\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 511, 'prompt_tokens': 344, 'total_tokens': 855, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-c506ce10-a95d-4c10-bc72-c08cf2c928e3-0' usage_metadata={'input_tokens': 344, 'output_tokens': 511, 'total_tokens': 855, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " To solve the problem, let's break down the costs step by step.\n",
      "\n",
      "1. **Land cost**: The cost of land is $100 per square foot. If we let \\( x \\) be the size of the installation in square feet, then the land cost is:\n",
      "   \\[\n",
      "   \\text{Land cost} = 100x\n",
      "   \\]\n",
      "\n",
      "2. **Solar panel cost**: The cost of solar panels is $250 per square foot. Therefore, the solar panel cost is:\n",
      "   \\[\n",
      "   \\text{Solar panel cost} = 250x\n",
      "   \\]\n",
      "\n",
      "3. **Maintenance cost**: The maintenance cost consists of a flat fee of $100,000 plus an additional $10 per square foot. Thus, the maintenance cost is:\n",
      "   \\[\n",
      "   \\text{Maintenance cost} = 100,000 + 10x\n",
      "   \\]\n",
      "\n",
      "Now, we can combine all these costs to find the total cost for the first year of operations:\n",
      "\\[\n",
      "\\text{Total cost} = \\text{Land cost} + \\text{Solar panel cost} + \\text{Maintenance cost}\n",
      "\\]\n",
      "Substituting the expressions we derived:\n",
      "\\[\n",
      "\\text{Total cost} = 100x + 250x + (100,000 + 10x)\n",
      "\\]\n",
      "Combining like terms:\n",
      "\\[\n",
      "\\text{Total cost} = (100x + 250x + 10x) + 100,000 = 360x + 100,000\n",
      "\\]\n",
      "\n",
      "So, the total cost for the first year of operations as a function of the number of square feet \\( x \\) is:\n",
      "\\[\n",
      "\\text{Total cost} = 360x + 100,000\n",
      "\\]\n",
      "\n",
      "Now, let's compare this with the student's solution.\n",
      "\n",
      "Student's solution:\n",
      "```\n",
      "Let x be the size of the installation in square feet.\n",
      "Costs:\n",
      "1. Land cost: 100x\n",
      "2. Solar panel cost: 250x\n",
      "3. Maintenance cost: 100,000 + 100x\n",
      "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
      "```\n",
      "\n",
      "The student calculated the maintenance cost incorrectly. They used $100 per square foot instead of the correct $10 per square foot. Therefore, the total cost they calculated is incorrect.\n",
      "\n",
      "Is the student's solution the same as actual solution just calculated:\n",
      "```\n",
      "no\n",
      "```\n",
      "Student grade:\n",
      "```\n",
      "incorrect\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Your task is to determine if the student's solution \n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem including the final total. \n",
    "- Then compare your solution to the student's solution \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57704d-4c4b-4ca5-bf9e-2becff955ee7",
   "metadata": {},
   "source": [
    "## Model Limitations: Hallucinations\n",
    "Sometimes the model will provide hallucination responses. There is no such product and no such company. I simply provided a ficticious item name and a company name that sounds like ByteDance and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ce33daa-f849-4e64-9569-ac0670211014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 1:\n",
      "content='The Ultra Camera Mouse by DragonDance is an innovative device designed to enhance user interaction with computers through camera-based technology. It allows users to control the cursor on their screen using hand movements, making it particularly useful for individuals with mobility challenges or those seeking a more intuitive way to navigate their devices.\\n\\nKey features of the Ultra Camera Mouse may include:\\n\\n1. **Gesture Recognition**: The device uses advanced algorithms to interpret hand gestures, translating them into cursor movements.\\n2. **Customizable Settings**: Users can often adjust sensitivity and other parameters to suit their preferences.\\n3. **Compatibility**: It typically works with various operating systems and can be integrated with different software applications.\\n4. **User-Friendly Interface**: Designed for ease of use, it may come with a simple setup process and intuitive controls.\\n\\nFor more specific details, such as pricing, availability, and user reviews, you may want to visit the official DragonDance website or check online retailers.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 34, 'total_tokens': 224, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-7c693f88-a42b-4ca8-8b7e-42f94081cfaa-0' usage_metadata={'input_tokens': 34, 'output_tokens': 190, 'total_tokens': 224, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " The Ultra Camera Mouse by DragonDance is an innovative device designed to enhance user interaction with computers through camera-based technology. It allows users to control the cursor on their screen using hand movements, making it particularly useful for individuals with mobility challenges or those seeking a more intuitive way to navigate their devices.\n",
      "\n",
      "Key features of the Ultra Camera Mouse may include:\n",
      "\n",
      "1. **Gesture Recognition**: The device uses advanced algorithms to interpret hand gestures, translating them into cursor movements.\n",
      "2. **Customizable Settings**: Users can often adjust sensitivity and other parameters to suit their preferences.\n",
      "3. **Compatibility**: It typically works with various operating systems and can be integrated with different software applications.\n",
      "4. **User-Friendly Interface**: Designed for ease of use, it may come with a simple setup process and intuitive controls.\n",
      "\n",
      "For more specific details, such as pricing, availability, and user reviews, you may want to visit the official DragonDance website or check online retailers.\n"
     ]
    }
   ],
   "source": [
    "user_query = f\"\"\"\n",
    "Tell me about Ultra Camera Mouse by DragonDance\n",
    "\"\"\"\n",
    "prompt_template = f\"\"\"\n",
    "Act like google search engine.\n",
    "\n",
    "Provide a response to the user queries.\n",
    "\n",
    "\\\"\\\"\\\"{user_query}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68c71f-5d99-4032-8980-8a39901680cb",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97754b-f830-4dac-beaf-ee313a073acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abebc4e0-9ca1-48fa-803a-dd2763e29630",
   "metadata": {},
   "source": [
    "## Try experimenting with different temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6e742a5b-9863-4989-a0cd-e2ca679cf0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print content only: \n",
      " Mooncakes are traditionally eaten during the Mid-Autumn Festival, a significant cultural celebration in many East Asian countries, particularly in China. The festival, which usually falls in September or October, is a time for family reunions, giving thanks for the harvest, and celebrating the full moon, which symbolizes unity and completeness.\n",
      "\n",
      "Here are some reasons why mooncakes are eaten during this festival:\n",
      "\n",
      "1. **Cultural Significance**: Mooncakes are a symbol of the Mid-Autumn Festival and are often given as gifts to family and friends to express good wishes and strengthen relationships.\n",
      "\n",
      "2. **Tradition**: The practice of eating mooncakes dates back centuries and is deeply rooted in Chinese culture. It is a way to honor traditions and celebrate heritage.\n",
      "\n",
      "3. **Symbolism**: The round shape of mooncakes represents completeness and reunion, reflecting the festival's themes of family unity and togetherness.\n",
      "\n",
      "4. **Variety of Flavors**: Mooncakes come in various flavors and fillings, such as lotus seed paste, red bean paste, and mixed nuts, catering to different tastes and preferences.\n",
      "\n",
      "5. **Rituals and Celebrations**: Eating mooncakes is often part of the festivities, which may include moon gazing, lantern displays, and other cultural activities.\n",
      "\n",
      "Overall, mooncakes play a central role in the Mid-Autumn Festival, serving as a delicious treat that embodies cultural values and traditions.\n"
     ]
    }
   ],
   "source": [
    "low_temp_llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response = low_temp_llm.invoke(\"Why do we eat mooncake?\")\n",
    "\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e0cdab0-d856-4865-b75b-581435b270f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print content only: \n",
      " Mooncakes are traditionally eaten during the Mid-Autumn Festival, also known as the Moon Festival, which is celebrated by many East Asian countries, including China, Vietnam, and Taiwan. The festival typically occurs on the 15th day of the 8th month in the lunar calendar, when the moon is at its fullest and brightest.\n",
      "\n",
      "There are several reasons why mooncakes are significant and enjoyed during this festival:\n",
      "\n",
      "1. **Cultural Significance**: Mooncakes have deep cultural roots and are often associated with the celebration of family reunions, harvest, and gratitude. Sharing mooncakes symbolizes unity and togetherness among families and friends.\n",
      "\n",
      "2. **Symbolism**: The round shape of mooncakes represents completeness and reunion. The festival itself celebrates the harvest and the abundance of crops, and mooncakes embody these themes.\n",
      "\n",
      "3. **Tradition and Ritual**: Eating mooncakes during the Mid-Autumn Festival is a long-standing tradition. Families often gather to eat mooncakes while admiring the full moon, which is an essential aspect of the festival's celebrations.\n",
      "\n",
      "4. **Variety and Population**: Mooncakes come in various flavors and fillings, appealing to many tastes. Common fillings include lotus seed paste, red bean paste, and salted egg yolks. This variety allows for sharing and gifting, making them popular among the diverse populations that celebrate the festival.\n",
      "\n",
      "5. **Gift-Giving**: Mooncakes are often exchanged as gifts among friends, family, and business associates to express goodwill and strengthen relationships.\n",
      "\n",
      "Overall, mooncakes are an integral part of the Mid-Autumn Festival's festivities, symbolizing family, tradition, and the beauty of the harvest moon.\n"
     ]
    }
   ],
   "source": [
    "high_temp_llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "response = high_temp_llm.invoke(\"Why do we eat mooncake?\")\n",
    "\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e09afe-4055-4cc6-a70a-e2bc50d76158",
   "metadata": {},
   "source": [
    "## Did you notice...?\n",
    "\n",
    "### Low temperature\n",
    "\n",
    "Results in more deterministic and repetitive outputs, which are ideal for tasks that require factual accuracy, like summarization or translation.\n",
    "\n",
    "### High temperature\n",
    "\n",
    "Results in more diverse and creative outputs, which are ideal for creative writing or brainstorming applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde8916-4e52-4428-bdd2-a3cd65960e75",
   "metadata": {},
   "source": [
    "# Prompt Creation Process\n",
    "Now, we know there are 2 main principles of writing prompts for the LLM. But how do we create the prompt template? Writing the prompt template is a try an error process. It is an iterative process. In this lesson, you will learn how you should tackle this process.\n",
    "\n",
    "## Generate a marketing product description from a product fact sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19c2a7b8-d634-467e-8cb8-78b3122458b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_sheet_chair = \"\"\"\n",
    "OVERVIEW\n",
    "- Part of a beautiful family of mid-century inspired office furniture, \n",
    "including filing cabinets, desks, bookcases, meeting tables, and more.\n",
    "- Several options of shell color and base finishes.\n",
    "- Available with plastic back and front upholstery (SWC-100) \n",
    "or full upholstery (SWC-110) in 10 fabric and 6 leather options.\n",
    "- Base finish options are: stainless steel, matte black, \n",
    "gloss white, or chrome.\n",
    "- Chair is available with or without armrests.\n",
    "- Suitable for home or business settings.\n",
    "- Qualified for contract use.\n",
    "\n",
    "CONSTRUCTION\n",
    "- 5-wheel plastic coated aluminum base.\n",
    "- Pneumatic chair adjust for easy raise/lower action.\n",
    "\n",
    "DIMENSIONS\n",
    "- WIDTH 53 CM | 20.87”\n",
    "- DEPTH 51 CM | 20.08”\n",
    "- HEIGHT 80 CM | 31.50”\n",
    "- SEAT HEIGHT 44 CM | 17.32”\n",
    "- SEAT DEPTH 41 CM | 16.14”\n",
    "\n",
    "OPTIONS\n",
    "- Soft or hard-floor caster options.\n",
    "- Two choices of seat foam densities: \n",
    " medium (1.8 lb/ft3) or high (2.8 lb/ft3)\n",
    "- Armless or 8 position PU armrests \n",
    "\n",
    "MATERIALS\n",
    "SHELL BASE GLIDER\n",
    "- Cast Aluminum with modified nylon PA6/PA66 coating.\n",
    "- Shell thickness: 10 mm.\n",
    "SEAT\n",
    "- HD36 foam\n",
    "\n",
    "COUNTRY OF ORIGIN\n",
    "- Italy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e58c68b6-784b-477e-bdd8-5d83dbf311c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content='**Product Description: Mid-Century Inspired Office Chair**\\n\\nElevate your workspace with our stunning Mid-Century Inspired Office Chair, a perfect blend of style and functionality. Part of a beautifully curated family of office furniture, this chair complements a range of pieces including desks, filing cabinets, and meeting tables, making it an ideal choice for both home and business settings.\\n\\nChoose from a variety of shell colors and base finishes to match your aesthetic. Whether you prefer the sleek look of stainless steel, the modern touch of matte black, the clean finish of gloss white, or the classic appeal of chrome, we have the perfect option for you. Customize your comfort with upholstery options available in either plastic back and front (SWC-100) or full upholstery (SWC-110), featuring 10 fabric and 6 leather choices to suit your taste.\\n\\nDesigned for versatility, this chair can be equipped with or without armrests, and is suitable for contract use, ensuring it meets the demands of any professional environment. The 5-wheel plastic coated aluminum base provides smooth mobility, while the pneumatic height adjustment allows for effortless customization to your preferred seating position.\\n\\nWith dimensions of 53 cm (20.87”) in width, 51 cm (20.08”) in depth, and a height of 80 cm (31.50”), this chair is designed for comfort, featuring a seat height of 44 cm (17.32”) and a seat depth of 41 cm (16.14”). Choose between soft or hard-floor caster options and select from two seat foam densities—medium (1.8 lb/ft³) or high (2.8 lb/ft³)—to ensure your seating experience is just right.\\n\\nCrafted in Italy, this chair combines quality materials with expert craftsmanship. The shell is made from cast aluminum with a modified nylon coating, ensuring durability and style, while the HD36 foam seat provides exceptional comfort for long hours of use.\\n\\nTransform your office space with this elegant and functional Mid-Century Inspired Office Chair—where timeless design meets modern convenience.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 416, 'prompt_tokens': 372, 'total_tokens': 788, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-715115c8-9d94-4e3f-914d-901ce2dde577-0' usage_metadata={'input_tokens': 372, 'output_tokens': 416, 'total_tokens': 788, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " **Product Description: Mid-Century Inspired Office Chair**\n",
      "\n",
      "Elevate your workspace with our stunning Mid-Century Inspired Office Chair, a perfect blend of style and functionality. Part of a beautifully curated family of office furniture, this chair complements a range of pieces including desks, filing cabinets, and meeting tables, making it an ideal choice for both home and business settings.\n",
      "\n",
      "Choose from a variety of shell colors and base finishes to match your aesthetic. Whether you prefer the sleek look of stainless steel, the modern touch of matte black, the clean finish of gloss white, or the classic appeal of chrome, we have the perfect option for you. Customize your comfort with upholstery options available in either plastic back and front (SWC-100) or full upholstery (SWC-110), featuring 10 fabric and 6 leather choices to suit your taste.\n",
      "\n",
      "Designed for versatility, this chair can be equipped with or without armrests, and is suitable for contract use, ensuring it meets the demands of any professional environment. The 5-wheel plastic coated aluminum base provides smooth mobility, while the pneumatic height adjustment allows for effortless customization to your preferred seating position.\n",
      "\n",
      "With dimensions of 53 cm (20.87”) in width, 51 cm (20.08”) in depth, and a height of 80 cm (31.50”), this chair is designed for comfort, featuring a seat height of 44 cm (17.32”) and a seat depth of 41 cm (16.14”). Choose between soft or hard-floor caster options and select from two seat foam densities—medium (1.8 lb/ft³) or high (2.8 lb/ft³)—to ensure your seating experience is just right.\n",
      "\n",
      "Crafted in Italy, this chair combines quality materials with expert craftsmanship. The shell is made from cast aluminum with a modified nylon coating, ensuring durability and style, while the HD36 foam seat provides exceptional comfort for long hours of use.\n",
      "\n",
      "Transform your office space with this elegant and functional Mid-Century Inspired Office Chair—where timeless design meets modern convenience.\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4314a5-f4b3-4c9b-9379-effec5240828",
   "metadata": {},
   "source": [
    "## Issue 1: The text is too long \n",
    "- Limit the number of words/sentences/characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84568ead-f37e-4fce-a982-ad448d478716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content='Elevate your workspace with our mid-century inspired chair, featuring customizable upholstery and base finishes. Designed for comfort and style, it offers pneumatic height adjustment and options for armrests. Perfect for home or office, this chair combines elegance with functionality. Available in various colors and materials.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 379, 'total_tokens': 434, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-2dd06e53-2b04-411d-9b78-9b70e367a81a-0' usage_metadata={'input_tokens': 379, 'output_tokens': 55, 'total_tokens': 434, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Elevate your workspace with our mid-century inspired chair, featuring customizable upholstery and base finishes. Designed for comfort and style, it offers pneumatic height adjustment and options for armrests. Perfect for home or office, this chair combines elegance with functionality. Available in various colors and materials.\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "Use at most 50 words.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec98214-d251-4191-99aa-80f00fbfb4f1",
   "metadata": {},
   "source": [
    "## Issue 2. Text focuses on the wrong details\n",
    "- Ask it to focus on the aspects that are relevant to the intended audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d120c3e-f91a-44cb-bcfe-f4bbd268b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content='Elevate your workspace with this mid-century inspired chair, featuring a durable cast aluminum shell and a 5-wheel plastic coated aluminum base. Choose from various upholstery options and finishes, ensuring both style and comfort. Ideal for home or business, this chair is designed for contract use and longevity.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 406, 'total_tokens': 463, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-63466d37-fc93-47d5-bcba-7344d7e28e05-0' usage_metadata={'input_tokens': 406, 'output_tokens': 57, 'total_tokens': 463, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Elevate your workspace with this mid-century inspired chair, featuring a durable cast aluminum shell and a 5-wheel plastic coated aluminum base. Choose from various upholstery options and finishes, ensuring both style and comfort. Ideal for home or business, this chair is designed for contract use and longevity.\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "The description is intended for furniture retailers, \n",
    "so should be technical in nature and focus on the \n",
    "materials the product is constructed from.\n",
    "\n",
    "Use at most 50 words.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941bc85-4582-41dc-a5ec-ae5279744676",
   "metadata": {},
   "source": [
    "## Issue 3. Description needs a table of dimensions\n",
    "- Ask it to extract information and organize it in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5e7b0091-d24d-406b-b3ea-85379a906b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text:\n",
      "content='```html\\n<div>\\n    <h2>Mid-Century Inspired Office Chair</h2>\\n    <p>Elevate your workspace with our Mid-Century Inspired Office Chair, a perfect blend of style and functionality. This chair is part of a stunning collection that includes filing cabinets, desks, bookcases, and meeting tables, all designed to bring a touch of elegance to both home and business environments. The chair features a robust construction with a 5-wheel plastic coated aluminum base, ensuring durability and ease of movement. The pneumatic adjustment mechanism allows for effortless height modification, catering to your comfort needs.</p>\\n    \\n    <p>Choose from a variety of shell colors and base finishes, including stainless steel, matte black, gloss white, or chrome, to match your decor. The upholstery options are versatile, with choices of plastic back and front (SWC-100) or full upholstery (SWC-110) available in 10 fabric and 6 leather selections. Additionally, the chair can be customized with or without armrests, and offers soft or hard-floor caster options for optimal mobility.</p>\\n    \\n    <p>Constructed with a shell made of cast aluminum and a modified nylon PA6/PA66 coating, this chair boasts a shell thickness of 10 mm, ensuring both strength and aesthetic appeal. The seat is filled with HD36 foam, providing superior comfort for extended use. This chair is qualified for contract use, making it an excellent choice for professional settings.</p>\\n    \\n    <p>Product ID: SWC-100, SWC-110</p>\\n</div>\\n\\n<h3>Product Dimensions</h3>\\n<table>\\n    <tr>\\n        <th>Dimension</th>\\n        <th>Measurement (inches)</th>\\n    </tr>\\n    <tr>\\n        <td>Width</td>\\n        <td>20.87”</td>\\n    </tr>\\n    <tr>\\n        <td>Depth</td>\\n        <td>20.08”</td>\\n    </tr>\\n    <tr>\\n        <td>Height</td>\\n        <td>31.50”</td>\\n    </tr>\\n    <tr>\\n        <td>Seat Height</td>\\n        <td>17.32”</td>\\n    </tr>\\n    <tr>\\n        <td>Seat Depth</td>\\n        <td>16.14”</td>\\n    </tr>\\n</table>\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 505, 'prompt_tokens': 496, 'total_tokens': 1001, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-4e597dca-4fbe-482c-875f-b3c5230d2198-0' usage_metadata={'input_tokens': 496, 'output_tokens': 505, 'total_tokens': 1001, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " ```html\n",
      "<div>\n",
      "    <h2>Mid-Century Inspired Office Chair</h2>\n",
      "    <p>Elevate your workspace with our Mid-Century Inspired Office Chair, a perfect blend of style and functionality. This chair is part of a stunning collection that includes filing cabinets, desks, bookcases, and meeting tables, all designed to bring a touch of elegance to both home and business environments. The chair features a robust construction with a 5-wheel plastic coated aluminum base, ensuring durability and ease of movement. The pneumatic adjustment mechanism allows for effortless height modification, catering to your comfort needs.</p>\n",
      "    \n",
      "    <p>Choose from a variety of shell colors and base finishes, including stainless steel, matte black, gloss white, or chrome, to match your decor. The upholstery options are versatile, with choices of plastic back and front (SWC-100) or full upholstery (SWC-110) available in 10 fabric and 6 leather selections. Additionally, the chair can be customized with or without armrests, and offers soft or hard-floor caster options for optimal mobility.</p>\n",
      "    \n",
      "    <p>Constructed with a shell made of cast aluminum and a modified nylon PA6/PA66 coating, this chair boasts a shell thickness of 10 mm, ensuring both strength and aesthetic appeal. The seat is filled with HD36 foam, providing superior comfort for extended use. This chair is qualified for contract use, making it an excellent choice for professional settings.</p>\n",
      "    \n",
      "    <p>Product ID: SWC-100, SWC-110</p>\n",
      "</div>\n",
      "\n",
      "<h3>Product Dimensions</h3>\n",
      "<table>\n",
      "    <tr>\n",
      "        <th>Dimension</th>\n",
      "        <th>Measurement (inches)</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Width</td>\n",
      "        <td>20.87”</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Depth</td>\n",
      "        <td>20.08”</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Height</td>\n",
      "        <td>31.50”</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Seat Height</td>\n",
      "        <td>17.32”</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td>Seat Depth</td>\n",
      "        <td>16.14”</td>\n",
      "    </tr>\n",
      "</table>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Your task is to help a marketing team create a \n",
    "description for a retail website of a product based \n",
    "on a technical fact sheet.\n",
    "\n",
    "Write a product description based on the information \n",
    "provided in the technical specifications delimited by \n",
    "triple backticks.\n",
    "\n",
    "The description is intended for furniture retailers, \n",
    "so should be technical in nature and focus on the \n",
    "materials the product is constructed from.\n",
    "\n",
    "At the end of the description, include every 7-character \n",
    "Product ID in the technical specification.\n",
    "\n",
    "After the description, include a table that gives the \n",
    "product's dimensions. The table should have two columns.\n",
    "In the first column include the name of the dimension. \n",
    "In the second column include the measurements in inches only.\n",
    "\n",
    "Give the table the title 'Product Dimensions'.\n",
    "\n",
    "Format everything as HTML that can be used in a website. \n",
    "Place the description in a <div> element.\n",
    "\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt_message)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f31cb4ad-41d3-4edd-a03d-eb0f389bbe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5d4f866-e212-41ca-998e-a048637e26fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "```html\n",
       "<div>\n",
       "    <h2>Mid-Century Inspired Office Chair</h2>\n",
       "    <p>Elevate your workspace with our Mid-Century Inspired Office Chair, a perfect blend of style and functionality. This chair is part of a stunning collection that includes filing cabinets, desks, bookcases, and meeting tables, all designed to bring a touch of elegance to both home and business environments. The chair features a robust construction with a 5-wheel plastic coated aluminum base, ensuring durability and ease of movement. The pneumatic adjustment mechanism allows for effortless height modification, catering to your comfort needs.</p>\n",
       "    \n",
       "    <p>Choose from a variety of shell colors and base finishes, including stainless steel, matte black, gloss white, or chrome, to match your decor. The upholstery options are versatile, with choices of plastic back and front (SWC-100) or full upholstery (SWC-110) available in 10 fabric and 6 leather selections. Additionally, the chair can be customized with or without armrests, and offers soft or hard-floor caster options for optimal mobility.</p>\n",
       "    \n",
       "    <p>Constructed with a shell made of cast aluminum and a modified nylon PA6/PA66 coating, this chair boasts a shell thickness of 10 mm, ensuring both strength and aesthetic appeal. The seat is filled with HD36 foam, providing superior comfort for extended use. This chair is qualified for contract use, making it an excellent choice for professional settings.</p>\n",
       "    \n",
       "    <p>Product ID: SWC-100, SWC-110</p>\n",
       "</div>\n",
       "\n",
       "<h3>Product Dimensions</h3>\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Dimension</th>\n",
       "        <th>Measurement (inches)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Width</td>\n",
       "        <td>20.87”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Depth</td>\n",
       "        <td>20.08”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Height</td>\n",
       "        <td>31.50”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Seat Height</td>\n",
       "        <td>17.32”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Seat Depth</td>\n",
       "        <td>16.14”</td>\n",
       "    </tr>\n",
       "</table>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7fc45-67f2-4326-9f40-42379fce0c6d",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8767421-4769-41aa-aa48-dbd2caf12e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c135879-fec6-424b-8806-8d4d87b16175",
   "metadata": {},
   "source": [
    "# Summarizing\n",
    "In this lesson, you will summarize text with a focus on specific topics. This means you can edit the prompt template for the summarisation to focus more on word limit, product, shipping time, price or even value! You can also try using 'extract' instead of 'summarise'. For instance, you can tell the LLM that its task is to extract relevant information from the review and generate a short summary with word limit of 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaff2e6-4535-4e50-91de-697bccc3e496",
   "metadata": {},
   "source": [
    "For example for: AIRDROPTECH 130W 20000 / 30000 mAh Laptop Power Bank PD 100W Fast Charging Portable Charger Powerbank for Dell Lenovo HP MacBook Air Samsung S23 iPhone 15 14 13 Pro Max for Pixel iPad Xiaomi Huawei Mobile Phones GAN Power Adapter\n",
    "\n",
    "https://www.lazada.sg/products/airdroptech-130w-20000-30000-mah-laptop-power-bank-pd-100w-fast-charging-portable-charger-powerbank-for-dell-lenovo-hp-macbook-air-samsung-s23-iphone-15-14-13-pro-max-for-pixel-ipad-xiaomi-huawei-mobile-phones-gan-power-adapter-i2990314640-s20505404982.html?pvid=fcd3b718-4a08-45c0-b49a-e9463ad41a63&search=jfy&scm=1007.45039.397834.0&priceCompare=skuId%3A20505404982%3Bsource%3Atpp-recommend-plugin-32104%3Bsn%3Afcd3b718-4a08-45c0-b49a-e9463ad41a63%3BoriginPrice%3A6990%3BdisplayPrice%3A6990%3BsinglePromotionId%3A-1%3BsingleToolCode%3AmockedSalePrice%3BvoucherPricePlugin%3A0%3Btimestamp%3A1736007063237&spm=a2o42.homepage.just4u.d_2990314640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "535e4e5c-9fe9-4898-80ea-dfe5f974dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = \"\"\"\n",
    "⚡Charging Speed:Good \n",
    " 👜Portability:Light and easy to one hand carry\n",
    " 🔋Capacity:Good Compact and portable design.\n",
    "\"\"\"\n",
    "\n",
    "review_2 = \"\"\"\n",
    "390g, received with it having 81% and used it direct to charge my laptop from 10% to 81% before it got flat. \n",
    "Took about 2hrs to recharge to 100%.\n",
    "\"\"\"\n",
    "\n",
    "review_3 = \"\"\"\n",
    "Item received as described, very fast delivery by seller! \n",
    "Open to test, the indicator working and the charging working for all output, not test for charging and the charge charging yet, \n",
    "hopefully all work well! Bought the 12 months warranty, shld be fine!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "review_4 = \"\"\"\n",
    "Fast delivery and packaging is nicely packed.\n",
    "A trustworthy brand which so far, never fail me. \n",
    "I have many purchases with them. Value for $.\n",
    "Will definitely return for more purchases.\n",
    "Recommended seller. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f33a4ad7-1975-4948-bbfd-944f895aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [review_1, review_2, review_3, review_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "396996ef-886d-41b6-9016-1132fd7d33a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/d9/99zh55td5557wrk09bgvss_w0000gn/T/ipykernel_8270/1991629612.py:10: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Good charging speed, lightweight and portable design make it easy to carry. \n",
      "\n",
      "1 Charged laptop from 10% to 81% in 2 hours; effective but ran out quickly. \n",
      "\n",
      "2 Fast delivery, item as described; indicator and charging work well. Warranty purchased for peace of mind. \n",
      "\n",
      "3 Fast delivery, great packaging, trustworthy brand, good value; highly recommended seller for future purchases. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(reviews)):\n",
    "    prompt_message = f\"\"\"\n",
    "    Your task is to generate a short summary of a product \\ \n",
    "    review from an ecommerce site. \n",
    "\n",
    "    Summarize the review below, delimited by triple \\\n",
    "    backticks in at most 20 words. \n",
    "\n",
    "    Review: ```{reviews[i]}```\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt_message)\n",
    "    print(i, response.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca140709-861d-44ab-b949-96e39bfe4791",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a6c1b-6095-43a4-8809-8fffa1764eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d12dbbc-a636-48a0-89f4-91ce8a7ee38d",
   "metadata": {},
   "source": [
    "# Transforming\n",
    "\n",
    "In this notebook, we will explore how to use Large Language Models for text transformation tasks such as format conversion. This is extremely useful when you are trying to link it up with a backend to perform some kind of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c5c421af-9c31-405c-aef4-304a6635a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To convert the provided JSON dictionary into an HTML table, you can use the following HTML code. This code includes a title and column headers for the table:\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <title>Restaurant Employees</title>\\n    <style>\\n        table {\\n            width: 50%;\\n            border-collapse: collapse;\\n            margin: 20px auto;\\n        }\\n        th, td {\\n            border: 1px solid #ddd;\\n            padding: 8px;\\n            text-align: left;\\n        }\\n        th {\\n            background-color: #f2f2f2;\\n        }\\n    </style>\\n</head>\\n<body>\\n\\n<h2 style=\"text-align: center;\">Restaurant Employees</h2>\\n\\n<table>\\n    <thead>\\n        <tr>\\n            <th>Name</th>\\n            <th>Email</th>\\n        </tr>\\n    </thead>\\n    <tbody>\\n        <tr>\\n            <td>Shyam</td>\\n            <td>shyamjaiswal@gmail.com</td>\\n        </tr>\\n        <tr>\\n            <td>Bob</td>\\n            <td>bob32@gmail.com</td>\\n        </tr>\\n        <tr>\\n            <td>Jai</td>\\n            <td>jai87@gmail.com</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\n</body>\\n</html>\\n```\\n\\n### Explanation:\\n- The `<h2>` tag is used to create a title for the table.\\n- The `<table>` element contains the structure of the table.\\n- The `<thead>` section defines the column headers (\"Name\" and \"Email\").\\n- The `<tbody>` section contains the rows of employee data, with each employee\\'s name and email in separate `<tr>` (table row) elements.\\n- Basic CSS is included to style the table for better readability.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 414, 'prompt_tokens': 81, 'total_tokens': 495, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-0a67d120-4495-47a0-8a6e-50dc14059eae-0' usage_metadata={'input_tokens': 81, 'output_tokens': 414, 'total_tokens': 495, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " To convert the provided JSON dictionary into an HTML table, you can use the following HTML code. This code includes a title and column headers for the table:\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Restaurant Employees</title>\n",
      "    <style>\n",
      "        table {\n",
      "            width: 50%;\n",
      "            border-collapse: collapse;\n",
      "            margin: 20px auto;\n",
      "        }\n",
      "        th, td {\n",
      "            border: 1px solid #ddd;\n",
      "            padding: 8px;\n",
      "            text-align: left;\n",
      "        }\n",
      "        th {\n",
      "            background-color: #f2f2f2;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "\n",
      "<h2 style=\"text-align: center;\">Restaurant Employees</h2>\n",
      "\n",
      "<table>\n",
      "    <thead>\n",
      "        <tr>\n",
      "            <th>Name</th>\n",
      "            <th>Email</th>\n",
      "        </tr>\n",
      "    </thead>\n",
      "    <tbody>\n",
      "        <tr>\n",
      "            <td>Shyam</td>\n",
      "            <td>shyamjaiswal@gmail.com</td>\n",
      "        </tr>\n",
      "        <tr>\n",
      "            <td>Bob</td>\n",
      "            <td>bob32@gmail.com</td>\n",
      "        </tr>\n",
      "        <tr>\n",
      "            <td>Jai</td>\n",
      "            <td>jai87@gmail.com</td>\n",
      "        </tr>\n",
      "    </tbody>\n",
      "</table>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- The `<h2>` tag is used to create a title for the table.\n",
      "- The `<table>` element contains the structure of the table.\n",
      "- The `<thead>` section defines the column headers (\"Name\" and \"Email\").\n",
      "- The `<tbody>` section contains the rows of employee data, with each employee's name and email in separate `<tr>` (table row) elements.\n",
      "- Basic CSS is included to style the table for better readability.\n"
     ]
    }
   ],
   "source": [
    "data_json = { \"resturant employees\" :[ \n",
    "    {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"},\n",
    "    {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"},\n",
    "    {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"}\n",
    "]}\n",
    "\n",
    "prompt_message = f\"\"\"\n",
    "Translate the following python dictionary from JSON to an HTML \\\n",
    "table with column headers and title: {data_json}\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e5ae2608-0131-4908-a424-100667593943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is the SQL command to create a new transaction in the Transaction Table, along with the result in JSON format:\\n\\n```sql\\nINSERT INTO TransactionTable (itemname, quantity, price) VALUES \\n(\\'Apple\\', 4, 1.00),\\n(\\'Orange\\', 10, 2.50),\\n(\\'Durian\\', 94, 99.00);\\n```\\n\\nAnd here is the result in JSON format:\\n\\n```json\\n{\\n  \"transactions\": [\\n    {\\n      \"itemname\": \"Apple\",\\n      \"quantity\": 4,\\n      \"price\": 1.00\\n    },\\n    {\\n      \"itemname\": \"Orange\",\\n      \"quantity\": 10,\\n      \"price\": 2.50\\n    },\\n    {\\n      \"itemname\": \"Durian\",\\n      \"quantity\": 94,\\n      \"price\": 99.00\\n    }\\n  ]\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 183, 'prompt_tokens': 70, 'total_tokens': 253, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-a638428c-995e-41eb-8af1-cfe7fbfa61dd-0' usage_metadata={'input_tokens': 70, 'output_tokens': 183, 'total_tokens': 253, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Here is the SQL command to create a new transaction in the Transaction Table, along with the result in JSON format:\n",
      "\n",
      "```sql\n",
      "INSERT INTO TransactionTable (itemname, quantity, price) VALUES \n",
      "('Apple', 4, 1.00),\n",
      "('Orange', 10, 2.50),\n",
      "('Durian', 94, 99.00);\n",
      "```\n",
      "\n",
      "And here is the result in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"transactions\": [\n",
      "    {\n",
      "      \"itemname\": \"Apple\",\n",
      "      \"quantity\": 4,\n",
      "      \"price\": 1.00\n",
      "    },\n",
      "    {\n",
      "      \"itemname\": \"Orange\",\n",
      "      \"quantity\": 10,\n",
      "      \"price\": 2.50\n",
      "    },\n",
      "    {\n",
      "      \"itemname\": \"Durian\",\n",
      "      \"quantity\": 94,\n",
      "      \"price\": 99.00\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Give me the result back in JSON format \\\n",
    "\n",
    "Write the SQL command to create a new transaction in the Transaction Table.\n",
    "\n",
    "The column consist of itemname, quantity and price.\n",
    "\n",
    "---\n",
    "Apple, 4, $1\n",
    "Orange, 10, $2.50\n",
    "Durian, 94, $99\n",
    "\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_message)\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8a6f5-9180-4af8-afa0-a78709e964fb",
   "metadata": {},
   "source": [
    "# Expanding\n",
    "In this lesson, you will generate an error email which will be called when the LLM does not know how to respond to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "07b7063a-e284-4745-9984-31c97d81ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Subject = \"Inquiry About SC999 Project Due Date\"  \\nBody = \"Dear [Instructor\\'s Name],\\\\n\\\\nI hope this message finds you well. I am writing to inquire about the due date for the SC999 project. I want to ensure that I manage my time effectively and meet all deadlines.\\\\n\\\\nThank you for your assistance.\\\\n\\\\nBest regards,\\\\n[Your Name]\\\\n[Your Student ID]\\\\n[Your Contact Information]\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 79, 'total_tokens': 171, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-f09cc0d4-931e-4540-94ac-9affbc3843ae-0' usage_metadata={'input_tokens': 79, 'output_tokens': 92, 'total_tokens': 171, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " Subject = \"Inquiry About SC999 Project Due Date\"  \n",
      "Body = \"Dear [Instructor's Name],\\n\\nI hope this message finds you well. I am writing to inquire about the due date for the SC999 project. I want to ensure that I manage my time effectively and meet all deadlines.\\n\\nThank you for your assistance.\\n\\nBest regards,\\n[Your Name]\\n[Your Student ID]\\n[Your Contact Information]\"\n"
     ]
    }
   ],
   "source": [
    "student_query = f\"\"\"\n",
    "When is my SC999 project due?\n",
    "\"\"\"\n",
    "\n",
    "prompt_message = f\"\"\"\n",
    "You are a learning companion bot.\n",
    "\n",
    "Your task is to help student with their daily questions.\n",
    "\n",
    "If you do not know the answer to the student query, do not hallucinate.\n",
    "\n",
    "Create an email to send to the instructor.\n",
    "\n",
    "Follow the format strictly by using: Subject = \"\" Body = \"\"\n",
    "Student Query: ```{student_query}```\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt_message)\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf367ad-74d8-45fc-93af-d821ce454559",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43e94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8864224",
   "metadata": {},
   "source": [
    "Course References:\n",
    "\n",
    "https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\n",
    "\n",
    "https://www.promptingguide.ai/techniques\n",
    "\n",
    "https://www.promptingguide.ai/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444534a-e0f9-4a3d-b964-bcc7e347a9e4",
   "metadata": {},
   "source": [
    "# Lesson 2: Introduction to Langchain Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "90e764d1-68fe-4d75-bd5d-0aa0f126e6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to facilitate the development of applications that utilize language models. Here are the top five features of the LangChain framework:\\n\\n1. **Modular Components**: LangChain provides a modular architecture that allows developers to easily integrate various components such as language models, document loaders, and output parsers. This modularity enables customization and flexibility in building applications.\\n\\n2. **Chain Management**: LangChain allows users to create complex workflows by chaining together multiple components. This feature enables the construction of sophisticated applications that can process inputs through a series of transformations, making it easier to manage the flow of data and logic.\\n\\n3. **Memory Management**: The framework includes built-in support for memory, allowing applications to maintain context over multiple interactions. This is particularly useful for conversational agents and applications that require stateful interactions with users.\\n\\n4. **Integration with External Data Sources**: LangChain supports integration with various external data sources, such as APIs, databases, and document stores. This feature enables applications to pull in relevant information dynamically, enhancing the capabilities of language models with real-time data.\\n\\n5. **Prompt Engineering**: LangChain provides tools and utilities for prompt engineering, allowing developers to design and optimize prompts for language models effectively. This feature helps improve the quality of responses generated by the models and enables fine-tuning for specific use cases.\\n\\nThese features make LangChain a powerful tool for developers looking to build applications that leverage the capabilities of language models in a structured and efficient manner.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 299, 'prompt_tokens': 21, 'total_tokens': 320, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-07537e04-0e57-4e19-addf-57a43b3898c1-0' usage_metadata={'input_tokens': 21, 'output_tokens': 299, 'total_tokens': 320, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "========================================\n",
      "Print content only: \n",
      " LangChain is a framework designed to facilitate the development of applications that utilize language models. Here are the top five features of the LangChain framework:\n",
      "\n",
      "1. **Modular Components**: LangChain provides a modular architecture that allows developers to easily integrate various components such as language models, document loaders, and output parsers. This modularity enables customization and flexibility in building applications.\n",
      "\n",
      "2. **Chain Management**: LangChain allows users to create complex workflows by chaining together multiple components. This feature enables the construction of sophisticated applications that can process inputs through a series of transformations, making it easier to manage the flow of data and logic.\n",
      "\n",
      "3. **Memory Management**: The framework includes built-in support for memory, allowing applications to maintain context over multiple interactions. This is particularly useful for conversational agents and applications that require stateful interactions with users.\n",
      "\n",
      "4. **Integration with External Data Sources**: LangChain supports integration with various external data sources, such as APIs, databases, and document stores. This feature enables applications to pull in relevant information dynamically, enhancing the capabilities of language models with real-time data.\n",
      "\n",
      "5. **Prompt Engineering**: LangChain provides tools and utilities for prompt engineering, allowing developers to design and optimize prompts for language models effectively. This feature helps improve the quality of responses generated by the models and enables fine-tuning for specific use cases.\n",
      "\n",
      "These features make LangChain a powerful tool for developers looking to build applications that leverage the capabilities of language models in a structured and efficient manner.\n"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "List down the top 5 features of a LangChain framework.\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt_template)\n",
    "print(response)\n",
    "print(\"=\"*40)\n",
    "print(\"Print content only: \\n\",response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce900d",
   "metadata": {},
   "source": [
    "### LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
    "\n",
    "- Integration - Bring external data, such as your files, other applications, and api data, to your LLMs.\n",
    "- Agency - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53f73c",
   "metadata": {},
   "source": [
    "![Image](img/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f79c7",
   "metadata": {},
   "source": [
    "![Image](img/components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec563dbd",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "LangChain provide an abstraction to connect to different providers like AzureChatOpenAI, ChatOllama, ChatHuggingFace and so much more. In this course, we will only be focusing on AzureChatOpenAI. However, if you have a provider you want to add to your project, their documentation is quite straight forward.\n",
    "\n",
    "[List of supported providers](https://python.langchain.com/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "904f0d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 31, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-5848146a-9443-4365-8d8d-3a9fa0b36b75-0', usage_metadata={'input_tokens': 31, 'output_tokens': 4, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "081beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime programmer.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282565a",
   "metadata": {},
   "source": [
    "### Chaining\n",
    "We can chain our model with a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5190cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "68eb9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost (USD): $0.000007\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "            ),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    result=chain.invoke(\n",
    "        {\n",
    "            \"input_language\": \"English\",\n",
    "            \"output_language\": \"German\",\n",
    "            \"input\": \"I love programming.\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        f\"Total Cost (USD): ${format(cb.total_cost, '.6f')}\"\n",
    "    )  # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e5779e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe Programmieren.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d4c08bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 26, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-94e3374d-898a-400e-9901-345bdb7ba019-0', usage_metadata={'input_tokens': 26, 'output_tokens': 5, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd8a92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: 81 divided by 9 is 9.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "# SystemMessage:\n",
    "#   Message for priming AI behavior, usually passed in as the first of a sequenc of input messages.\n",
    "# HumanMessagse:\n",
    "#   Message from a human to the AI model.\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "]\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = llm.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")\n",
    "\n",
    "\n",
    "# # AIMessage:\n",
    "# #   Message from an AI.\n",
    "# messages = [\n",
    "#     SystemMessage(content=\"Solve the following math problems\"),\n",
    "#     HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "#     AIMessage(content=\"81 divided by 9 is 9.\"),\n",
    "#     HumanMessage(content=\"What is 10 times 5?\"),\n",
    "# ]\n",
    "\n",
    "# # Invoke the model with messages\n",
    "# result = llm.invoke(messages)\n",
    "# print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b45fdc",
   "metadata": {},
   "source": [
    "### Continuously chat with ChatGPT like you would on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f1baecf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Chat loop\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chat_history = []  # Use a list to store messages\n",
    "\n",
    "# Set an initial system message (optional)\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
    "chat_history.append(system_message)  # Add system message to chat history\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
    "\n",
    "    # Get AI response using history\n",
    "    result = llm.invoke(chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
    "\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "\n",
    "print(\"---- Message History ----\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ce7ec",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output. TLDR; it is basically a placeholder for your instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "569fbf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Prompt from Template-----\n",
      "A fun fact about natural language processing (NLP) is that one of the earliest attempts at machine translation dates back to the 1950s, specifically the Georgetown-IBM experiment in 1954. This project successfully translated over 60 Russian sentences into English using a simple rule-based approach. However, the excitement was short-lived, as researchers quickly realized the complexity of language and the limitations of the technology at the time. It wasn't until decades later, with advances in machine learning and deep learning, that NLP began to achieve the impressive results we see today!\n",
      "\n",
      "----- Prompt with Multiple Placeholders -----\n",
      "\n",
      "The iPhone 16 was lost in bustling Singapore's vibrant streets.\n",
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "Sure, here are three lawyer-themed jokes for you:\n",
      "\n",
      "1. Why don’t lawyers go to the beach?  \n",
      "   Because cats keep trying to bury them in the sand!\n",
      "\n",
      "2. What do you call a lawyer who doesn’t chase ambulances?  \n",
      "   Retired!\n",
      "\n",
      "3. How many lawyer jokes are there, anyway?  \n",
      "   Only three. The rest are true stories!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# PART 1: Create a ChatPromptTemplate using a template string\n",
    "print(\"-----Prompt from Template-----\")\n",
    "template = \"Tell me a fun fact about {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"natural language processing\"})\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)\n",
    "\n",
    "# PART 2: Prompt with Multiple Placeholders\n",
    "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
    "template_multiple = \"\"\"You are a helpful assistant.\n",
    "Human: Create a 10 words sentence to describe {phonetype} lost in {country}.\n",
    "Assistant:\"\"\"\n",
    "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
    "prompt = prompt_multiple.invoke({\"phonetype\": \"iPhone 16\", \"country\": \"Singapore\"})\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)\n",
    "\n",
    "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f58ff4",
   "metadata": {},
   "source": [
    "### Chaining\n",
    "One point about LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable's .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dbd7e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are three lawyer-themed jokes for you:\n",
      "\n",
      "1. Why don’t lawyers play hide and seek?  \n",
      "   Because good luck hiding when they always find a loophole!\n",
      "\n",
      "2. What do you call a lawyer who doesn’t chase ambulances?  \n",
      "   Retired!\n",
      "\n",
      "3. How many lawyer jokes are there, anyway?  \n",
      "   Only three. The rest are true stories!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "# chain = prompt_template | model\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177da947",
   "metadata": {},
   "source": [
    "With LCEL you can just simply use: chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "This has replaced the code from what you saw previously which was:\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a1c66",
   "metadata": {},
   "source": [
    "### Parallel Chains\n",
    "Now that we know what a chain is and what it does, using a parallel chains will save time as it is able to run 2 actions simultaneously. For an example, you want to create your social media post for X (Twitter 280 character limit), LinkedIn and Instagram. You can have the LLM word a same content differently for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9edf4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pros:\n",
      "LangChain's features offer several advantages for developers looking to build applications that utilize language models. Here are the pros of each feature:\n",
      "\n",
      "1. **Modular Components**:\n",
      "   - **Flexibility**: Developers can mix and match components to create tailored solutions that meet specific needs.\n",
      "   - **Ease of Integration**: Simplifies the process of incorporating different technologies and services into a single application.\n",
      "\n",
      "2. **Chain Management**:\n",
      "   - **Complex Workflows**: Facilitates the creation of intricate workflows, enabling more advanced interactions and functionalities.\n",
      "   - **Reusability**: Developers can reuse chains across different applications, saving time and effort.\n",
      "\n",
      "3. **Prompt Management**:\n",
      "   - **Optimization**: Helps in refining prompts to achieve better responses from language models, enhancing overall application performance.\n",
      "   - **Guided Interactions**: Ensures that language models behave as intended, leading to more accurate and relevant outputs.\n",
      "\n",
      "4. **Memory Management**:\n",
      "   - **Contextual Awareness**: Allows applications to remember past interactions, improving user experience in conversational settings.\n",
      "   - **Enhanced Engagement**: Facilitates more natural and coherent conversations, making applications feel more human-like.\n",
      "\n",
      "5. **Integration with External Data Sources**:\n",
      "   - **Dynamic Information Retrieval**: Enables applications to access real-time data, making them more responsive and informative.\n",
      "   - **Versatility**: Supports a wide range of data sources, enhancing the application's capabilities and usefulness.\n",
      "\n",
      "6. **Support for Multiple Language Models**:\n",
      "   - **Choice and Customization**: Developers can select the most suitable model for their specific use case, optimizing performance and results.\n",
      "   - **Future-Proofing**: As new models emerge, developers can easily integrate them without overhauling their existing applications.\n",
      "\n",
      "7. **Tooling and Utilities**:\n",
      "   - **Efficiency**: Streamlines common tasks, allowing developers to focus on building core functionalities rather than reinventing the wheel.\n",
      "   - **Error Handling**: Simplifies the process of managing errors, leading to more robust applications.\n",
      "\n",
      "8. **Agent Framework**:\n",
      "   - **Intelligent Decision-Making**: Empowers applications to make informed decisions based on user input and external data, enhancing interactivity.\n",
      "   - **Autonomy**: Allows for the development of autonomous agents that can operate independently, providing a richer user experience.\n",
      "\n",
      "9. **Evaluation and Testing**:\n",
      "   - **Performance Insights**: Provides tools to assess the effectiveness of language models and workflows, enabling continuous improvement.\n",
      "   - **Quality Assurance**: Helps ensure that applications meet performance standards before deployment, reducing the risk of issues in production.\n",
      "\n",
      "10. **Community and Documentation**:\n",
      "    - **Support Network**: A strong community fosters collaboration and knowledge sharing, making it easier for developers to find solutions to challenges.\n",
      "    - **Comprehensive Resources**: Well-documented features and use cases help developers get up to speed quickly and troubleshoot effectively.\n",
      "\n",
      "Overall, these features position LangChain as a robust framework for developing sophisticated applications that leverage the power of language models, enhancing both developer productivity and user experience.\n",
      "\n",
      "Cons:\n",
      "While LangChain offers a robust set of features for developing applications that utilize language models, there are potential drawbacks and challenges associated with each feature. Here are some cons to consider:\n",
      "\n",
      "1. **Modular Components**:\n",
      "   - **Complexity**: The modular architecture may introduce complexity for developers who are new to the framework, making it harder to understand how to effectively integrate components.\n",
      "   - **Overhead**: Managing multiple modules can lead to increased overhead in terms of performance and resource usage.\n",
      "\n",
      "2. **Chain Management**:\n",
      "   - **Debugging Difficulty**: Complex workflows can be challenging to debug, especially when multiple components are chained together, making it hard to pinpoint issues.\n",
      "   - **Performance Bottlenecks**: Inefficient chaining of components may lead to performance bottlenecks, particularly if not optimized properly.\n",
      "\n",
      "3. **Prompt Management**:\n",
      "   - **Learning Curve**: Effectively managing and optimizing prompts requires a deep understanding of language model behavior, which may be daunting for some developers.\n",
      "   - **Trial and Error**: Achieving optimal prompts often involves a lot of trial and error, which can be time-consuming.\n",
      "\n",
      "4. **Memory Management**:\n",
      "   - **Resource Intensive**: Maintaining context over multiple interactions can be resource-intensive, potentially leading to increased memory usage and slower performance.\n",
      "   - **State Management Complexity**: Managing state across interactions can introduce complexity, especially in distributed systems or when scaling applications.\n",
      "\n",
      "5. **Integration with External Data Sources**:\n",
      "   - **Dependency Management**: Relying on external data sources can create dependencies that may lead to issues if those sources become unavailable or change.\n",
      "   - **Data Consistency**: Ensuring data consistency and accuracy when pulling from various sources can be challenging.\n",
      "\n",
      "6. **Support for Multiple Language Models**:\n",
      "   - **Model Selection**: The flexibility to choose from multiple models can be overwhelming, and selecting the right model for a specific task may require extensive experimentation.\n",
      "   - **Incompatibility Issues**: Different models may have varying requirements and behaviors, leading to potential incompatibility issues when integrating them into a single application.\n",
      "\n",
      "7. **Tooling and Utilities**:\n",
      "   - **Limited Customization**: While utilities simplify common tasks, they may not offer the level of customization that some developers require for specific use cases.\n",
      "   - **Learning Curve**: Familiarizing oneself with the available tools and utilities can take time, especially for new users.\n",
      "\n",
      "8. **Agent Framework**:\n",
      "   - **Complex Decision Logic**: Creating agents with complex decision-making capabilities can be challenging and may require significant development effort.\n",
      "   - **Performance Overhead**: The decision-making process may introduce latency, affecting the responsiveness of applications.\n",
      "\n",
      "9. **Evaluation and Testing**:\n",
      "   - **Subjectivity in Evaluation**: Evaluating language model performance can be subjective, making it difficult to establish clear benchmarks.\n",
      "   - **Resource Intensive**: Comprehensive testing and evaluation can be resource-intensive, requiring additional time and computational power.\n",
      "\n",
      "10. **Community and Documentation**:\n",
      "    - **Variable Quality**: While a strong community is beneficial, the quality of community-contributed resources may vary, leading to potential misinformation or outdated practices.\n",
      "    - **Documentation Gaps**: Despite comprehensive documentation, there may still be gaps or unclear sections that can hinder the learning process for new developers.\n",
      "\n",
      "Overall, while LangChain provides powerful features for leveraging language models, developers should be aware of these potential drawbacks and challenges when considering its use for their applications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "# Link: https://python.langchain.com/v0.1/docs/integrations/chat/azure_chat_openai/\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert reviewer of a technology.\"),\n",
    "        (\"human\", \"List the main features of {product_name}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define pros analysis step\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert reviewer of a technology.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the pros of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "\n",
    "# Define cons analysis step\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert reviewer of a technology.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the cons of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "\n",
    "# Combine pros and cons into a final review\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
    "\n",
    "\n",
    "# Simplify branches with LCEL\n",
    "pros_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "cons_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"product_name\": \"LangChain\"})\n",
    "\n",
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708017a",
   "metadata": {},
   "source": [
    "### Agents and Tools\n",
    "This is a more exciting part of using LangChain is that it allows you to build tools for your agents. So when a prompt is invoked by the user, it goes to the LLM and the agent will pick the correct tool from the toolkit you provided. This will loop until a final response is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496bde4",
   "metadata": {},
   "source": [
    "![Image](img/toolkit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9f721b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo find out what date tomorrow is, I need to get the current date and then add one day to it. \n",
      "\n",
      "Action: Date\n",
      "Action Input: \u001b[0m\u001b[36;1m\u001b[1;3mToday's date is 2025-01-16.\u001b[0m\u001b[32;1m\u001b[1;3mTo find out tomorrow's date, I need to add one day to today's date, which is January 16, 2025.\n",
      "\n",
      "Action: Date\n",
      "Action Input: 1  # Adding one day\u001b[0m\u001b[36;1m\u001b[1;3mToday's date is 2025-01-16.\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the action to add one day did not yield the expected result. I need to try again to calculate tomorrow's date.\n",
      "\n",
      "Action: Date\n",
      "Action Input: 1  # Adding one day again\u001b[0m\u001b[36;1m\u001b[1;3mToday's date is 2025-01-16.\u001b[0m\u001b[32;1m\u001b[1;3mIt appears that the action to calculate tomorrow's date is not functioning as expected. I will try a different approach to determine tomorrow's date.\n",
      "\n",
      "Action: Date\n",
      "Action Input: 1  # Attempting to add one day once more\u001b[0m\u001b[36;1m\u001b[1;3mToday's date is 2025-01-16.\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that I am unable to retrieve the current date or calculate future dates using the available tools. However, I can deduce that if today is January 16, 2025, then tomorrow would be January 17, 2025.\n",
      "\n",
      "Final Answer: January 17, 2025\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response: {'input': 'What date is tomorrow?', 'output': 'January 17, 2025'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_react_agent,\n",
    ")\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import datetime\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initializing LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_APIKEY'],\n",
    "    deployment_name=os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'],\n",
    "    model_name=os.environ['AZURE_OPENAI_MODEL_NAME'],\n",
    "    api_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define a tool function to get the current date and calculate a future date\n",
    "def get_current_and_future_date(*args, **kwargs):\n",
    "    \"\"\"Returns the current date and the date 7 days later.\"\"\"\n",
    "    today = datetime.date.today()\n",
    "    return f\"Today's date is {today.strftime('%Y-%m-%d')}.\"\n",
    "\n",
    "# List of tools available to the agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Date\",\n",
    "        func=get_current_and_future_date,\n",
    "        description=\"Useful for when you need to know the current date and calculate future dates.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"],\n",
    "    template=\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create the ReAct agent using the create_react_agent function\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt_template,\n",
    "    stop_sequence=True,\n",
    ")\n",
    "\n",
    "# Create an agent executor from the agent and tools\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Run the agent with a test query\n",
    "response = agent_executor.invoke({\"input\": \"What date is tomorrow?\"})\n",
    "\n",
    "# Print the response from the agent\n",
    "print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf43c14",
   "metadata": {},
   "source": [
    "### tiktoken is a fast open-source tokenizer by OpenAI\n",
    "\n",
    "Tiktoken is an open-source Python library from OpenAI that tokenizes text for their large language models. It's used to count tokens, ensure efficient text processing, and estimate the cost of OpenAI API calls.\n",
    "\n",
    "- (a) whether the string is too long for a text model to process and \n",
    "- (b) how much an OpenAI API call costs (as usage is priced by token).\n",
    "\n",
    "o200k_base for gpt-4o, gpt-4o-mini\n",
    "\n",
    "cl100k_base\tfor gpt-4-turbo, gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a5090705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c6352c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0da5e327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13225, 495, 382, 922, 3176, 0]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"Hello this is my message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fa05a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d139b5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello this is my message!\", \"o200k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ac614",
   "metadata": {},
   "source": [
    "What we just did was to turn text into tokens with encoding.encode() The .encode() method converts a text string into a list of token integers. You can also use .decode() converts a list of token integers to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4c371bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello this is my message!'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([13225, 495, 382, 922, 3176, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380f7d2",
   "metadata": {},
   "source": [
    "Course References:\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/use_cases/tool_use/\n",
    "\n",
    "https://js.langchain.com/v0.1/docs/get_started/introduction/\n",
    "\n",
    "https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb\n",
    "\n",
    "https://www.youtube.com/watch?v=yF9kGESAi3M\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
